{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUdjG4-XsE0I"
      },
      "source": [
        "# **ColabProTrek**\n",
        "\n",
        "<a href=\"https://www.biorxiv.org/content/10.1101/2024.05.30.596740v1\"><img src=\"https://img.shields.io/badge/Paper-bioRxiv-green\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://huggingface.co/ProTrekHub\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-red?label=ProTrekHub\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://huggingface.co/spaces/westlake-repl/Demo_ProTrek_650M_UniRef50\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-red?label=Demo\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://huggingface.co/westlake-repl/ProTrek_650M_UniRef50\"><img src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-yellow?label=Model\" style=\"max-width: 100%;\"></a>\n",
        "<a href=\"https://cbirt.net/charting-the-protein-universe-with-protreks-tri-modal-contrastive-learning/\" alt=\"blog\"><img src=\"https://img.shields.io/badge/Blog-Medium-purple\" /></a>\n",
        "\n",
        "\n",
        "This is **ColabProTrek**, the Colab version of [ProTrek](https://github.com/westlake-repl/ProTrek) [(paper)](https://www.biorxiv.org/content/10.1101/2024.05.30.596740v1)\n",
        "\n",
        "ProTrek is also a retrieval model for sequence-structure-function pairwise searches. [Try](https://huggingface.co/spaces/westlake-repl/Demo_ProTrek_650M_UniRef50)\n",
        "\n",
        "**ColabProTrek** is a platform where **Protein Language Models(PLMs)** are more accessible and user-friendly for biologists, enabling effortless model training and sharing within the scientific community.\n",
        "\n",
        "ColabProTrek is a member of the OPMC family. You might also be interested in its sibling, [ColabSaprot](https://colab.research.google.com/github/westlake-repl/SaprotHub/blob/main/colab/SaprotHub_v2.ipynb), [ColabProtT5](https://colab.research.google.com/github/westlake-repl/SaprotHub/blob/main/colab/ColabProtT5.ipynb)\n",
        "\n",
        "<!-- If you find our ColabProTrek useful for your research, please also consider citing our [OPMC literature](https://www.biorxiv.org/content/10.1101/2024.05.24.595648v4). We have invested significant effort in developing these platforms. -->\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nLb_im9sJWw"
      },
      "source": [
        "## ColabProTrek\n",
        "\n",
        "| Function                             | Tutorial                                                     | Video                                                        |\n",
        "| ------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
        "| <a href=\"#train\">Train your model</a>                     | [How to train your model](https://github.com/westlake-repl/SaprotHub/wiki/ColabProTrek-&-ColabProtT5#21-Train-your-model) | -  |\n",
        "| <a href=\"#prediction\">Classification/Regression Prediction</a> | [How to use model for classification/regression prediction](https://github.com/westlake-repl/SaprotHub/wiki/ColabProTrek-&-ColabProtT5#31-Classification-Regression-Prediction) |\n",
        "\n",
        "<br>\n",
        "\n",
        "<font color=red>**To view the content, please click on the first option in the left sidebar.**</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-dw1U1uBI7d"
      },
      "source": [
        "# **1: Installation**\n",
        "\n",
        "## ‚ö†Ô∏èSWITCH YOUR RUNTIME TYPE TO GPU\n",
        "Before installing ProTrek, please **<font color=red>SWITCH YOUR RUNTIME TYPE TO GPU!!!</font>**\n",
        "\n",
        "> üìçPlease check this [page](https://github.com/westlake-repl/SaprotHub/wiki/ColabProTrek-&-ColabProtT5#11-Switch-your-runtime-type-to-GPU) to learn **how to switch your runtime type to GPU**.\n",
        "\n",
        "## ‚ö†Ô∏èMaximum Runtime and Idle Timeout\n",
        "\n",
        "To ensure your program finishes properly, please avoid letting your computer go to **<font color=red>sleep</font>** or remain **<font color=red>idle</font>** for long periods.\n",
        "\n",
        "Please be aware of **<font color=red>the maximum runtime</font>**, as your program may be automatically terminated when this limit is reached.\n",
        "\n",
        "| Plan            | Maximum Runtime | Idle Timeout | Additional Features                        |\n",
        "|-----------------|------------------|--------------|--------------------------------------------|\n",
        "| **Free**        | 12 hours         | Yes          | -                                          |\n",
        "| **Colab Pro**   | Based on availability and usage patterns | Yes          | Increased compute availability             |\n",
        "| **Pay As You Go**| Based on availability and usage patterns | Yes          | Increased compute availability             |\n",
        "| **Colab Pro+**  | Up to 24 hours   | No           | Background execution, continuous code execution |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Tgvb8ibwBI7d"
      },
      "outputs": [],
      "source": [
        "#@title **1.1: ‚ñ∂Ô∏è Click the run button to install ProTrek**\n",
        "\n",
        "#@markdown (Please waiting for 2-8 minutes to install...)\n",
        "################################################################################\n",
        "########################### install saprot #####################################\n",
        "################################################################################\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "import os\n",
        "# Check whether the server is local or from google cloud\n",
        "root_dir = os.getcwd()\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "try:\n",
        "  import sys\n",
        "  sys.path.append(f\"{root_dir}/SaprotHub\")\n",
        "  import saprot\n",
        "  print(\"ProTrek is installed successfully!\")\n",
        "  os.system(f\"chmod +x {root_dir}/SaprotHub/bin/*\")\n",
        "\n",
        "except ImportError:\n",
        "  print(\"Installing ProTrek...\")\n",
        "  os.system(f\"rm -rf {root_dir}/SaprotHub\")\n",
        "  # !rm -rf /content/SaprotHub/\n",
        "\n",
        "  !echo \"Cloning into 'ProTrekHub'...\"\n",
        "  !git clone https://github.com/westlake-repl/SaprotHub.git > /dev/null 2>&1\n",
        "  !pip install huggingface_hub=0.23.2 > /dev/null 2>&1\n",
        "\n",
        "  # !pip install /content/SaprotHub/saprot-0.4.7-py3-none-any.whl\n",
        "  os.system(f\"pip install -r {root_dir}/SaprotHub/requirements.txt\")\n",
        "  # !pip install -r /content/SaprotHub/requirements.txt\n",
        "\n",
        "  os.system(f\"pip install {root_dir}/SaprotHub\")\n",
        "\n",
        "\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/LMDB\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/bin\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/output\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/datasets\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/regression/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/token_classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/pair_classification/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/adapters/pair_regression/Local\")\n",
        "  os.system(f\"mkdir -p {root_dir}/SaprotHub/structures\")\n",
        "  # !mkdir -p /content/SaprotHub/LMDB\n",
        "  # !mkdir -p /content/SaprotHub/bin\n",
        "  # !mkdir -p /content/SaprotHub/output\n",
        "  # !mkdir -p /content/SaprotHub/datasets\n",
        "  # !mkdir -p /content/SaprotHub/adapters/classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/regression/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/token_classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/pair_classification/Local\n",
        "  # !mkdir -p /content/SaprotHub/adapters/pair_regression/Local\n",
        "  # !mkdir -p /content/SaprotHub/structures\n",
        "\n",
        "  # !pip install gdown==v4.6.3 --force-reinstall --quiet\n",
        "  # os.system(\n",
        "  #   f\"wget 'https://drive.usercontent.google.com/download?id=1B_9t3n_nlj8Y3Kpc_mMjtMdY0OPYa7Re&export=download&authuser=0' -O {root_dir}/SaprotHub/bin/foldseek\"\n",
        "  # )\n",
        "\n",
        "  os.system(f\"chmod +x {root_dir}/SaprotHub/bin/*\")\n",
        "  # !chmod +x /content/SaprotHub/bin/foldseek\n",
        "  import sys\n",
        "  sys.path.append(f\"{root_dir}/SaprotHub\")\n",
        "\n",
        "\n",
        "  # IMPORTANT!!!! Used to fix the error caused by the mismatch of the versions of third-party libraries!!\n",
        "  import matplotlib.pyplot as plt\n",
        "  plt.figure(figsize=(1, 1))\n",
        "  plt.plot([], [], marker='o', linestyle='-', color='b')\n",
        "  plt.show()\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################## global ######################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "# IMPORTANT!!!! Used to fix the error caused by the mismatch of the versions of third-party libraries!!\n",
        "import sys\n",
        "keys=[]\n",
        "for k in sys.modules.keys():\n",
        "  for sub_str in [\"numpy\"]:\n",
        "    if sub_str in k:\n",
        "      keys.append(k)\n",
        "for k in keys:\n",
        "  del sys.modules[k]\n",
        "\n",
        "from transformers import AutoTokenizer, EsmForProteinFolding, EsmTokenizer\n",
        "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
        "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
        "import ipywidgets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import lmdb\n",
        "import base64\n",
        "import copy\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import yaml\n",
        "import argparse\n",
        "import pprint\n",
        "import subprocess\n",
        "import py3Dmol\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "from loguru import logger\n",
        "from easydict import EasyDict\n",
        "from colorama import init, Fore, Back, Style\n",
        "from IPython.display import clear_output\n",
        "from saprot.utils.mpr import MultipleProcessRunnerSimplifier\n",
        "from huggingface_hub import snapshot_download\n",
        "from ipywidgets import HTML\n",
        "from IPython.display import display\n",
        "from google.colab import widgets\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from string import ascii_uppercase,ascii_lowercase\n",
        "from saprot.data.parse import get_chain_ids\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DATASET_HOME = Path(f'{root_dir}/SaprotHub/datasets')\n",
        "ADAPTER_HOME = Path(f'{root_dir}/SaprotHub/adapters')\n",
        "STRUCTURE_HOME = Path(f\"{root_dir}/SaprotHub/structures\")\n",
        "LMDB_HOME = Path(f'{root_dir}/SaprotHub/LMDB')\n",
        "OUTPUT_HOME = Path(f'{root_dir}/SaprotHub/output')\n",
        "UPLOAD_FILE_HOME = Path(f'{root_dir}/SaprotHub/upload_files')\n",
        "FOLDSEEK_PATH = Path(f\"{root_dir}/SaprotHub/bin/foldseek\")\n",
        "aa_set = {\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"}\n",
        "foldseek_struc_vocab = \"pynwrqhgdlvtmfsaeikc#\"\n",
        "\n",
        "data_type_list = [\"Single AA Sequence\",\n",
        "                  \"Single SA Sequence\",\n",
        "                  \"Single UniProt ID\",\n",
        "                  \"Single PDB/CIF Structure\",\n",
        "                  \"Multiple AA Sequences\",\n",
        "                  \"Multiple SA Sequences\",\n",
        "                  \"Multiple UniProt IDs\",\n",
        "                  \"Multiple PDB/CIF Structures\",\n",
        "                  \"SaprotHub Dataset\",\n",
        "                  \"A pair of AA Sequences\",\n",
        "                  \"A pair of SA Sequences\",\n",
        "                  \"A pair of UniProt IDs\",\n",
        "                  \"A pair of PDB/CIF Structures\",\n",
        "                  \"Multiple pairs of AA Sequences\",\n",
        "                  \"Multiple pairs of SA Sequences\",\n",
        "                  \"Multiple pairs of UniProt IDs\",\n",
        "                  \"Multiple pairs of PDB/CIF Structures\",]\n",
        "\n",
        "data_type_list_single = [\n",
        "    \"Single AA Sequence\",\n",
        "    \"Single SA Sequence\",\n",
        "    \"Single UniProt ID\",\n",
        "    \"Single PDB/CIF Structure\",\n",
        "    \"A pair of AA Sequences\",\n",
        "    \"A pair of SA Sequences\",\n",
        "    \"A pair of UniProt IDs\",\n",
        "    \"A pair of PDB/CIF Structures\",]\n",
        "\n",
        "data_type_list_multiple = [\n",
        "    \"Multiple AA Sequences\",\n",
        "    \"Multiple SA Sequences\",\n",
        "    \"Multiple UniProt IDs\",\n",
        "    \"Multiple PDB/CIF Structures\",\n",
        "    \"Multiple pairs of AA Sequences\",\n",
        "    \"Multiple pairs of SA Sequences\",\n",
        "    \"Multiple pairs of UniProt IDs\",\n",
        "    \"Multiple pairs of PDB/CIF Structures\",]\n",
        "\n",
        "task_type_dict = {\n",
        "  \"Protein-level Classification\": \"classification\",\n",
        "  \"Residue-level Classification\" : \"token_classification\",\n",
        "  \"Protein-level Regression\" : \"regression\",\n",
        "  \"Protein-protein Classification\": \"pair_classification\",\n",
        "  \"Protein-protein Regression\": \"pair_regression\",\n",
        "}\n",
        "model_type_dict = {\n",
        "  \"classification\" : \"saprot/saprot_classification_model\",\n",
        "  \"token_classification\" : \"saprot/saprot_token_classification_model\",\n",
        "  \"regression\" : \"saprot/saprot_regression_model\",\n",
        "  \"pair_classification\" : \"saprot/saprot_pair_classification_model\",\n",
        "  \"pair_regression\" : \"saprot/saprot_pair_regression_model\",\n",
        "}\n",
        "dataset_type_dict = {\n",
        "  \"classification\": \"saprot/saprot_classification_dataset\",\n",
        "  \"token_classification\" : \"saprot/saprot_token_classification_dataset\",\n",
        "  \"regression\": \"saprot/saprot_regression_dataset\",\n",
        "  \"pair_classification\" : \"saprot/saprot_pair_classification_dataset\",\n",
        "  \"pair_regression\" : \"saprot/saprot_pair_regression_dataset\",\n",
        "}\n",
        "training_data_type_dict = {\n",
        "  \"Single AA Sequence\": \"AA\",\n",
        "  \"Single SA Sequence\": \"SA\",\n",
        "  \"Single UniProt ID\": \"SA\",\n",
        "  \"Single PDB/CIF Structure\": \"SA\",\n",
        "  \"Multiple AA Sequences\": \"AA\",\n",
        "  \"Multiple SA Sequences\": \"SA\",\n",
        "  \"Multiple UniProt IDs\": \"SA\",\n",
        "  \"Multiple PDB/CIF Structures\": \"SA\",\n",
        "  \"SaprotHub Dataset\": \"SA\",\n",
        "  \"A pair of AA Sequences\": \"AA\",\n",
        "  \"A pair of SA Sequences\": \"SA\",\n",
        "  \"A pair of UniProt IDs\": \"SA\",\n",
        "  \"A pair of PDB/CIF Structures\": \"SA\",\n",
        "  \"Multiple pairs of AA Sequences\": \"AA\",\n",
        "  \"Multiple pairs of SA Sequences\": \"SA\",\n",
        "  \"Multiple pairs of UniProt IDs\": \"SA\",\n",
        "  \"Multiple pairs of PDB/CIF Structures\": \"SA\",\n",
        "}\n",
        "\n",
        "\n",
        "class font:\n",
        "    RED = '\\033[91m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    BLUE = '\\033[94m'\n",
        "\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "    RESET = '\\033[0m'\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################### adapters #######################################\n",
        "################################################################################\n",
        "def get_adapters_list(task_type=None):\n",
        "\n",
        "    adapters_list = []\n",
        "\n",
        "    if task_type:\n",
        "      for file_path in (ADAPTER_HOME / task_type).glob('**/adapter_config.json'):\n",
        "        adapters_list.append(file_path.parent)\n",
        "    else:\n",
        "      for file_path in ADAPTER_HOME.glob('**/adapter_config.json'):\n",
        "        adapters_list.append(file_path.parent)\n",
        "\n",
        "    return adapters_list\n",
        "\n",
        "def adapters_text(adapters_list):\n",
        "  input = ipywidgets.Text(\n",
        "    value=None,\n",
        "    placeholder='Enter ProTrekHub Model ID',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  input.layout.width = '500px'\n",
        "  display(input)\n",
        "\n",
        "  return input\n",
        "\n",
        "def adapters_dropdown(adapters_list):\n",
        "  dropdown = ipywidgets.Dropdown(\n",
        "    options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "    value=None,\n",
        "    placeholder='Select a Local Model here',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  dropdown.layout.width = '500px'\n",
        "  display(dropdown)\n",
        "\n",
        "  return dropdown\n",
        "\n",
        "def adapters_combobox(adapters_list):\n",
        "  combobox = ipywidgets.Combobox(\n",
        "    options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "    value=None,\n",
        "    placeholder='Enter ProTrekHub Model repository id or select a Local Model here',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  combobox.layout.width = '500px'\n",
        "  display(combobox)\n",
        "\n",
        "  return combobox\n",
        "\n",
        "def adapters_selectmultiple(adapters_list):\n",
        "  selectmulitiple = ipywidgets.SelectMultiple(\n",
        "  options=[f\"{adapter_path.parent.stem}/{adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "  value=[],\n",
        "  #rows=10,\n",
        "  placeholder='Select multiple models',\n",
        "  # description='Fruits',\n",
        "  disabled=False,\n",
        "  layout={'width': '500px'})\n",
        "  display(selectmulitiple)\n",
        "\n",
        "  return selectmulitiple\n",
        "\n",
        "def adapters_textmultiple(adapters_list):\n",
        "  textmultiple = ipywidgets.Text(\n",
        "  value=None,\n",
        "  placeholder='Enter multiple ProTrekHub Model IDs, separated by commas.',\n",
        "  # description='Fruits',\n",
        "  disabled=False,\n",
        "  layout={'width': '500px'})\n",
        "  display(textmultiple)\n",
        "\n",
        "  return textmultiple\n",
        "\n",
        "\n",
        "def select_adapter_from(task_type, use_model_from):\n",
        "  adapters_list = get_adapters_list(task_type)\n",
        "\n",
        "  if use_model_from == 'Trained by yourself on ColabProTrek':\n",
        "    print(Fore.BLUE+f\"Local Model ({task_type}):\"+Style.RESET_ALL)\n",
        "    return adapters_dropdown(adapters_list)\n",
        "\n",
        "  elif use_model_from == 'Shared by peers on ProTrekHub':\n",
        "    print(Fore.BLUE+\"ProTrekHub Model:\"+Style.RESET_ALL)\n",
        "    return adapters_text(adapters_list)\n",
        "\n",
        "  elif use_model_from == \"Saved in your local computer\":\n",
        "    print(Fore.BLUE+\"Click the button to upload the \\\"Model-<task_name>-<model_size>.zip\\\" file of your Model:\"+Style.RESET_ALL)\n",
        "    # 1. upload model.zip\n",
        "    adapter_upload_path = ADAPTER_HOME / task_type / \"Local\"\n",
        "    adapter_zip_path = upload_file(adapter_upload_path)\n",
        "    adapter_path = adapter_upload_path / adapter_zip_path.stem\n",
        "    # 2. unzip model.zip\n",
        "    with zipfile.ZipFile(adapter_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(adapter_path)\n",
        "    os.remove(adapter_zip_path)\n",
        "    # 3. check adapter_config.json\n",
        "    adapter_config_path = adapter_path / \"adapter_config.json\"\n",
        "    assert adapter_config_path.exists(), f\"Can't find {adapter_config_path}\"\n",
        "\n",
        "    return EasyDict({\"value\":  f\"Local/{adapter_zip_path.stem}\"})\n",
        "\n",
        "  elif use_model_from == \"Multi-models on ColabProTrek\":\n",
        "    # 1. select the list of adapters\n",
        "    print(Fore.BLUE+f\"Local Model ({task_type}):\"+Style.RESET_ALL)\n",
        "    print(Fore.BLUE+f\"Multiple values can be selected with \\\"shift\\\" and/or \\\"ctrl\\\" (or \\\"command\\\") pressed and mouse clicks or arrow keys.\"+Style.RESET_ALL)\n",
        "    return adapters_selectmultiple(adapters_list)\n",
        "\n",
        "  elif use_model_from == \"Multi-models on ProTrekHub\":\n",
        "    # 1. enter the list of adapters\n",
        "    print(Fore.BLUE+f\"ProTrekHub Model IDs, separated by commas ({task_type}):\"+Style.RESET_ALL)\n",
        "    return adapters_textmultiple(adapters_list)\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################### download dataset ###################################\n",
        "################################################################################\n",
        "def download_dataset(task_name):\n",
        "  import gdown\n",
        "  import tarfile\n",
        "\n",
        "  filepath = LMDB_HOME / f\"{task_name}.tar.gz\"\n",
        "  download_links = {\n",
        "    \"ClinVar\" : \"https://drive.google.com/uc?id=1Le6-v8ddXa1eLJZFo7HPij7NhaBmNUbo\",\n",
        "    \"DeepLoc_cls2\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"DeepLoc_cls10\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"EC\" : \"https://drive.google.com/uc?id=1VFLFA-jK1tkTZBVbMw8YSsjZqAqlVQVQ\",\n",
        "    \"GO_BP\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_CC\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_MF\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"HumanPPI\" : \"https://drive.google.com/uc?id=1ahgj-IQTtv3Ib5iaiXO_ASh2hskEsvoX\",\n",
        "    \"MetalIonBinding\" : \"https://drive.google.com/uc?id=1rwknPWIHrXKQoiYvgQy4Jd-efspY16x3\",\n",
        "    \"ProteinGym\" : \"https://drive.google.com/uc?id=1L-ODrhfeSjDom-kQ2JNDa2nDEpS8EGfD\",\n",
        "    \"Thermostability\" : \"https://drive.google.com/uc?id=1I9GR1stFDHc8W3FCsiykyrkNprDyUzSz\",\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    gdown.download(download_links[task_name], str(filepath), quiet=False)\n",
        "    with tarfile.open(filepath, 'r:gz') as tar:\n",
        "      tar.extractall(path=str(LMDB_HOME))\n",
        "      print(f\"Extracted: {filepath}\")\n",
        "  except Exception as e:\n",
        "    raise RuntimeError(\"The dataset has not prepared.\")\n",
        "\n",
        "################################################################################\n",
        "############################# upload file ######################################\n",
        "################################################################################\n",
        "def upload_file(upload_path):\n",
        "  import shutil\n",
        "  import os\n",
        "  from pathlib import Path\n",
        "  import sys\n",
        "\n",
        "  upload_path = Path(upload_path)\n",
        "  upload_path.mkdir(parents=True, exist_ok=True)\n",
        "  basepath = Path().resolve()\n",
        "  try:\n",
        "    uploaded = files.upload()\n",
        "    filenames = []\n",
        "    for filename in uploaded.keys():\n",
        "      filenames.append(filename)\n",
        "      shutil.move(basepath / filename, upload_path / filename)\n",
        "    if len(filenames) == 0:\n",
        "      logger.info(\"The uploading process has been interrupted by the user.\")\n",
        "      raise RuntimeError(\"The uploading process has been interrupted by the user.\")\n",
        "  except Exception as e:\n",
        "    logger.error(\"Upload file fail! Please click the button to run again.\")\n",
        "    raise(e)\n",
        "\n",
        "  return upload_path / filenames[0]\n",
        "\n",
        "################################################################################\n",
        "############################ upload dataset ####################################\n",
        "################################################################################\n",
        "\n",
        "def read_csv_dataset(uploaded_csv_path):\n",
        "  df = pd.read_csv(uploaded_csv_path)\n",
        "  df.columns = df.columns.str.lower()\n",
        "  return df\n",
        "\n",
        "def check_column_label_and_stage(csv_dataset_path):\n",
        "  df = read_csv_dataset(csv_dataset_path)\n",
        "  for column in df.columns:\n",
        "    if 'sequence' in column:\n",
        "        df[column] = df[column].apply(lambda x: ''.join([c for c in x if not (c.islower() or c == '#')]))\n",
        "  assert {'label', 'stage'}.issubset(df.columns), f\"Make sure your CSV dataset includes both `label` and `stage` columns!\\nCurrent columns: {df.columns}\"\n",
        "  column_values = set(df['stage'].unique())\n",
        "  assert all(value in column_values for value in ['train', 'valid', 'test']), f\"Ensure your dataset includes samples for all three stages: `train`, `valid` and `test`.\\nCurrent columns: {df.columns}\"\n",
        "  output_file = os.path.join(os.path.dirname(csv_dataset_path), 'cleaned_dataset.csv')\n",
        "  df.to_csv(output_file, index=False)\n",
        "\n",
        "  return output_file\n",
        "\n",
        "def get_data_type(csv_dataset_path):\n",
        "  # AA, SA, Pair AA, Pair SA\n",
        "  df = read_csv_dataset(csv_dataset_path)\n",
        "  df = df.rename(columns={\n",
        "    \"protein_1\": \"sequence_1\",\n",
        "    \"protein_2\": \"sequence_2\",\n",
        "    \"protein\": \"sequence\"})\n",
        "\n",
        "  # AA, SA\n",
        "  if 'sequence' in df.columns:\n",
        "    second_token = df.loc[0, 'sequence'][1]\n",
        "    if second_token in aa_set:\n",
        "      return \"Multiple AA Sequences\"\n",
        "    elif second_token in foldseek_struc_vocab:\n",
        "      return \"Multiple SA Sequences\"\n",
        "    else:\n",
        "      raise RuntimeError(f\"The sequence in the dataset({csv_dataset_path}) are neither SA Sequences nor AA Sequences. Please check carefully.\")\n",
        "\n",
        "  # Pair AA, Pair SA\n",
        "  elif 'sequence_1' in df.columns and 'sequence_2' in df.columns:\n",
        "    second_token = df.loc[0, 'sequence_1'][1]\n",
        "    if second_token in aa_set:\n",
        "      return \"Multiple pairs of AA Sequences\"\n",
        "    elif second_token in foldseek_struc_vocab:\n",
        "      return \"Multiple pairs of SA Sequences\"\n",
        "    else:\n",
        "      raise RuntimeError(f\"The sequence in the dataset({csv_dataset_path}) are neither SA Sequences nor AA Sequences. Please check carefully.\")\n",
        "\n",
        "  else:\n",
        "      raise RuntimeError(f\"The data type of the dataset({csv_dataset_path}) should be one of the following types: Multiple AA Sequences, Multiple SA Sequences, Multiple pairs of AA Sequences, Multiple pairs of SA Sequences\")\n",
        "\n",
        "def check_task_type_and_data_type(original_task_type, data_type):\n",
        "  if \"Protein-protein\" in original_task_type:\n",
        "    assert data_type == \"SaprotHub Dataset\" or \"pair\" in data_type, f\"The current `data_type`({data_type}) is incompatible with the current `task_type`({original_task_type}). Please use Pair Sequence Datset for {original_task_type} task!\"\n",
        "  else:\n",
        "    assert \"pair\" not in data_type, f\"The current `data_type`({data_type}) is incompatible with the current `task_type`({original_task_type}). Please avoid using the Pair Sequence Dataset({data_type}) for the {original_task_type} task!\"\n",
        "\n",
        "def input_raw_data_by_data_type(data_type):\n",
        "  print(Fore.BLUE+\"Dataset: \"+Style.RESET_ALL, end='')\n",
        "\n",
        "  # 0-2. 0. Single AA Sequence, 1. Single SA Sequence, 2. Single UniProt ID\n",
        "  if data_type in data_type_list[:3]:\n",
        "    input_seq = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter {data_type} here',\n",
        "      disabled=False)\n",
        "    input_seq.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"{data_type}\"+Style.RESET_ALL)\n",
        "    display(input_seq)\n",
        "    return input_seq\n",
        "\n",
        "  # 3. Single PDB/CIF Structure\n",
        "  elif data_type == 'Single PDB/CIF Structure':\n",
        "    print(\"Please provide the structure type, chain and your structure file.\")\n",
        "\n",
        "    dropdown_type = ipywidgets.Dropdown(\n",
        "      value=\"AF2\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type)\n",
        "\n",
        "    input_chain = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain here',\n",
        "      disabled=False)\n",
        "    input_chain.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain:\"+Style.RESET_ALL)\n",
        "    display(input_chain)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path = upload_file(STRUCTURE_HOME)\n",
        "    return pdb_file_path.stem, dropdown_type, input_chain\n",
        "\n",
        "  # 4-7 & 13-16. Multiple Sequences\n",
        "  elif data_type in data_type_list_multiple:\n",
        "    print(Fore.BLUE+f\"Please upload the .csv file which contains {data_type}\"+Style.RESET_ALL)\n",
        "    uploaded_csv_path = upload_file(UPLOAD_FILE_HOME)\n",
        "    print(Fore.BLUE+\"Successfully upload your .csv file!\"+Style.RESET_ALL)\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    if data_type in ['Multiple PDB/CIF Structures', 'Multiple pairs of PDB/CIF Structures']:\n",
        "      # upload and unzip PDB files\n",
        "      print(Fore.BLUE+f\"Please upload your .zip file which contains {data_type} files\"+Style.RESET_ALL)\n",
        "      pdb_zip_path = upload_file(UPLOAD_FILE_HOME)\n",
        "      if pdb_zip_path.suffix != \".zip\":\n",
        "        logger.error(\"The data type does not match. Please click the run button again to upload a .zip file!\")\n",
        "        raise RuntimeError(\"The data type does not match.\")\n",
        "      print(Fore.BLUE+\"Successfully upload your .zip file!\"+Style.RESET_ALL)\n",
        "      print(\"=\"*100)\n",
        "\n",
        "      import zipfile\n",
        "      with zipfile.ZipFile(pdb_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(STRUCTURE_HOME)\n",
        "\n",
        "    return uploaded_csv_path\n",
        "\n",
        "  # 8. SaprotHub Dataset\n",
        "  elif data_type == \"SaprotHub Dataset\":\n",
        "    input_repo_id = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Copy and paste the SaprotHub Dataset ID here',\n",
        "      disabled=False)\n",
        "    input_repo_id.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"{data_type}\"+Style.RESET_ALL)\n",
        "    display(input_repo_id)\n",
        "    return input_repo_id\n",
        "\n",
        "  # 9-11. A pair of seq\n",
        "  elif data_type in [\"A pair of AA Sequences\", \"A pair of SA Sequences\", \"A pair of UniProt IDs\"]:\n",
        "    print()\n",
        "\n",
        "    seq_type = data_type[len(\"A pair of \"):-1]\n",
        "\n",
        "    input_seq1 = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter the {seq_type} of Sequence 1 here',\n",
        "      disabled=False)\n",
        "    input_seq1.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"Sequence 1:\"+Style.RESET_ALL)\n",
        "    display(input_seq1)\n",
        "\n",
        "    input_seq2 = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder=f'Enter the {seq_type} of Sequence 2 here',\n",
        "      disabled=False)\n",
        "    input_seq2.layout.width = '500px'\n",
        "    print(Fore.BLUE+f\"Sequence 2:\"+Style.RESET_ALL)\n",
        "    display(input_seq2)\n",
        "\n",
        "    return (input_seq1, input_seq2)\n",
        "\n",
        "  # 12. Pair Single PDB/CIF Structure\n",
        "  elif data_type == 'A pair of PDB/CIF Structures':\n",
        "    print(\"Please provide the structure type, chain and your structure file.\")\n",
        "\n",
        "    dropdown_type1 = ipywidgets.Dropdown(\n",
        "      value=\"PDB\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type1.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"The first structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type1)\n",
        "\n",
        "    input_chain1 = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain of the first structure here',\n",
        "      disabled=False)\n",
        "    input_chain1.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain of the first structure:\"+Style.RESET_ALL)\n",
        "    display(input_chain1)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path1 = upload_file(STRUCTURE_HOME)\n",
        "\n",
        "\n",
        "    dropdown_type2 = ipywidgets.Dropdown(\n",
        "      value=\"PDB\",\n",
        "      options=[\"PDB\", \"AF2\"],\n",
        "      disabled=False)\n",
        "    dropdown_type2.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"The second structure type:\"+Style.RESET_ALL)\n",
        "    display(dropdown_type2)\n",
        "\n",
        "    input_chain2 = ipywidgets.Text(\n",
        "      value=\"A\",\n",
        "      placeholder=f'Enter the name of chain of the second structure here',\n",
        "      disabled=False)\n",
        "    input_chain2.layout.width = '500px'\n",
        "    print(Fore.BLUE+\"Chain of the second structure:\"+Style.RESET_ALL)\n",
        "    display(input_chain2)\n",
        "\n",
        "    print(Fore.BLUE+\"Please upload a .pdb/.cif file\"+Style.RESET_ALL)\n",
        "    pdb_file_path2 = upload_file(STRUCTURE_HOME)\n",
        "    return (pdb_file_path1.stem, dropdown_type1, input_chain1, pdb_file_path2.stem, dropdown_type2, input_chain2)\n",
        "\n",
        "def get_SA_sequence_by_data_type(data_type, raw_data):\n",
        "\n",
        "  # Multiple sequences\n",
        "  # raw_data = upload_files/xxx.csv\n",
        "\n",
        "  # 8. SaprotHub Dataset\n",
        "  if data_type == \"SaprotHub Dataset\":\n",
        "    input_repo_id = raw_data\n",
        "    REPO_ID = input_repo_id.value\n",
        "\n",
        "    if REPO_ID.startswith('/'):\n",
        "      return Path(REPO_ID)\n",
        "\n",
        "    snapshot_download(repo_id=REPO_ID, repo_type=\"dataset\", local_dir=DATASET_HOME / REPO_ID)\n",
        "    csv_dataset_path = DATASET_HOME / REPO_ID / 'dataset.csv'\n",
        "    assert csv_dataset_path.exists(), f\"Can't find {csv_dataset_path}\"\n",
        "    protein_df = read_csv_dataset(csv_dataset_path)\n",
        "\n",
        "    data_type = get_data_type(csv_dataset_path)\n",
        "\n",
        "    return get_SA_sequence_by_data_type(data_type, csv_dataset_path)\n",
        "\n",
        "    # # AA, SA\n",
        "    # if data_type == \"Multiple AA Sequences\":\n",
        "    #   for index, value in protein_df['sequence'].items():\n",
        "    #     sa_seq = ''\n",
        "    #     for aa in value:\n",
        "    #       sa_seq += aa + '#'\n",
        "    #     protein_df.at[index, 'sequence'] = sa_seq\n",
        "\n",
        "    # # Pair AA, Pair SA\n",
        "    # elif data_type in [\"Multiple pairs of AA Sequences\", \"Multiple pairs of SA Sequences\"]:\n",
        "    #   for i in ['1', '2']:\n",
        "    #     if data_type == \"Multiple pairs of AA Sequences\":\n",
        "    #       for index, value in protein_df[f'sequence_{i}'].items():\n",
        "    #         sa_seq = ''\n",
        "    #         for aa in value:\n",
        "    #           sa_seq += aa + '#'\n",
        "    #         protein_df.at[index, f'sequence_{i}'] = sa_seq\n",
        "\n",
        "    #     protein_df[f'name_{i}'] = f'name_{i}'\n",
        "    #     protein_df[f'chain_{i}'] = 'A'\n",
        "\n",
        "    # protein_df.to_csv(csv_dataset_path, index=None)\n",
        "\n",
        "    # return csv_dataset_path\n",
        "\n",
        "  elif data_type in data_type_list_multiple:\n",
        "    uploaded_csv_path = raw_data\n",
        "    csv_dataset_path = DATASET_HOME / uploaded_csv_path.name\n",
        "    protein_df = read_csv_dataset(uploaded_csv_path)\n",
        "    protein_df = protein_df.rename(columns={\n",
        "    \"protein_1\": \"sequence_1\",\n",
        "    \"protein_2\": \"sequence_2\",\n",
        "    \"protein\": \"sequence\"})\n",
        "\n",
        "    if 'pair' in data_type:\n",
        "      assert {'sequence_1', 'sequence_2'}.issubset(protein_df.columns), f\"The CSV dataset ({uploaded_csv_path}) must contain `sequence_1` and `sequence_2` columns. \\n Current columns:{protein_df.columns}\"\n",
        "    else:\n",
        "      assert 'sequence' in protein_df.columns, f\"The CSV Dataset({uploaded_csv_path}) must contain a `sequence` column. \\n Current columns:{protein_df.columns}\"\n",
        "\n",
        "    # 4. Multiple AA Sequences\n",
        "    if data_type == 'Multiple AA Sequences':\n",
        "      for index, value in protein_df['sequence'].items():\n",
        "        sa_seq = ''\n",
        "        for aa in value:\n",
        "          sa_seq += aa + '#'\n",
        "        protein_df.at[index, 'sequence'] = sa_seq\n",
        "\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 5. Multiple SA Sequences\n",
        "    elif data_type == 'Multiple SA Sequences':\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 6. Multiple UniProt IDs\n",
        "    elif data_type == 'Multiple UniProt IDs':\n",
        "      protein_list = protein_df.loc[:, 'sequence'].tolist()\n",
        "      uniprot2pdb(protein_list)\n",
        "      protein_list = [(uniprot_id, \"AF2\", \"A\") for uniprot_id in protein_list]\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      outputs = mprs.run()\n",
        "\n",
        "      protein_df['sequence'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 7. Multiple PDB/CIF Structures\n",
        "    elif data_type == 'Multiple PDB/CIF Structures':\n",
        "      # protein_list = [(uniprot_id, type, chain), ...]\n",
        "      # protein_list = [item.split('.')[0] for item in protein_df.iloc[:, 0].tolist()]\n",
        "      # uniprot2pdb(protein_list)\n",
        "      protein_list = []\n",
        "      for row_tuple in protein_df.itertuples(index=False):\n",
        "        assert row_tuple.type in ['PDB', 'AF2'],  \"The type of structure must be either \\\"PDB\\\" or \\\"AF2\\\"!\"\n",
        "        protein_list.append(row_tuple)\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      outputs = mprs.run()\n",
        "\n",
        "      protein_df['sequence'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 13. Pair Multiple AA Sequences\n",
        "    elif data_type == \"Multiple pairs of AA Sequences\":\n",
        "      for i in ['1', '2']:\n",
        "        for index, value in protein_df[f'sequence_{i}'].items():\n",
        "          sa_seq = ''\n",
        "          for aa in value:\n",
        "            sa_seq += aa + '#'\n",
        "          protein_df.at[index, f'sequence_{i}'] = sa_seq\n",
        "\n",
        "        protein_df[f'name_{i}'] = f'name_{i}'\n",
        "        protein_df[f'chain_{i}'] = 'A'\n",
        "\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 14. Pair Multiple SA Sequences\n",
        "    elif data_type == \"Multiple pairs of SA Sequences\":\n",
        "      for i in ['1', '2']:\n",
        "        protein_df[f'name_{i}'] = f'name_{i}'\n",
        "        protein_df[f'chain_{i}'] = 'A'\n",
        "\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    # 15. Pair Multiple UniProt IDs\n",
        "    elif data_type == \"Multiple pairs of UniProt IDs\":\n",
        "      for i in ['1', '2']:\n",
        "        protein_list = protein_df.loc[:, f'sequence_{i}'].tolist()\n",
        "        uniprot2pdb(protein_list)\n",
        "        protein_df[f'name_{i}'] = protein_list\n",
        "        protein_list = [(uniprot_id, \"AF2\", \"A\") for uniprot_id in protein_list]\n",
        "        mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "        outputs = mprs.run()\n",
        "\n",
        "        protein_df[f'sequence_{i}'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "        protein_df[f'chain_{i}'] = 'A'\n",
        "\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "    elif data_type ==  \"Multiple pairs of PDB/CIF Structures\":\n",
        "      # columns: sequence_1, sequence_2, type_1, type_2, chain_1, chain_2, label, stage\n",
        "\n",
        "      # protein_list = [(uniprot_id, type, chain), ...]\n",
        "      # protein_list = [item.split('.')[0] for item in protein_df.iloc[:, 0].tolist()]\n",
        "      # uniprot2pdb(protein_list)\n",
        "\n",
        "      for i in ['1', '2']:\n",
        "        protein_list = []\n",
        "        for index, row in protein_df.iterrows():\n",
        "          assert row[f\"type_{i}\"] in ['PDB', 'AF2'],  \"The type of structure must be either \\\"PDB\\\" or \\\"AF2\\\"!\"\n",
        "          row_tuple = (row[f\"sequence_{i}\"], row[f\"type_{i}\"], row[f\"chain_{i}\"])\n",
        "          protein_list.append(row_tuple)\n",
        "        mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "        outputs = mprs.run()\n",
        "\n",
        "        # add name column, del type column\n",
        "        protein_df[f'name_{i}'] = protein_df[f'sequence_{i}'].apply(lambda x: x.split('.')[0])\n",
        "        protein_df.drop(f\"type_{i}\", axis=1, inplace=True)\n",
        "        protein_df[f'sequence_{i}'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "\n",
        "      # columns: name_1, name_2, chain_1, chain_2, sequence_1, sequence_2, label, stage\n",
        "      protein_df.to_csv(csv_dataset_path, index=None)\n",
        "      return csv_dataset_path\n",
        "\n",
        "  else:\n",
        "    # 0. Single AA Sequence\n",
        "    if data_type == 'Single AA Sequence':\n",
        "      input_seq = raw_data\n",
        "      aa_seq = input_seq.value\n",
        "\n",
        "      sa_seq = ''\n",
        "      for aa in aa_seq:\n",
        "          sa_seq += aa + '#'\n",
        "      return sa_seq\n",
        "\n",
        "    # 1. Single SA Sequence\n",
        "    elif data_type == 'Single SA Sequence':\n",
        "      input_seq = raw_data\n",
        "      sa_seq = input_seq.value\n",
        "\n",
        "      return sa_seq\n",
        "\n",
        "    # 2. Single UniProt ID\n",
        "    elif data_type == 'Single UniProt ID':\n",
        "      input_seq = raw_data\n",
        "      uniprot_id = input_seq.value\n",
        "\n",
        "\n",
        "      protein_list = [(uniprot_id, \"AF2\", \"A\")]\n",
        "      uniprot2pdb([protein_list[0][0]])\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      seqs = mprs.run()\n",
        "      sa_seq = seqs[0].split('\\t')[1]\n",
        "      return sa_seq\n",
        "\n",
        "    # 3. Single PDB/CIF Structure\n",
        "    elif data_type == 'Single PDB/CIF Structure':\n",
        "      uniprot_id = raw_data[0]\n",
        "      struc_type = raw_data[1].value\n",
        "      chain = raw_data[2].value\n",
        "\n",
        "      protein_list = [(uniprot_id, struc_type, chain)]\n",
        "      mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=2, return_results=True)\n",
        "      seqs = mprs.run()\n",
        "      assert len(seqs)>0, \"Unable to convert to SA sequence. Please check the `type`, `chain`, and `.pdb/.cif file`.\"\n",
        "      sa_seq = seqs[0].split('\\t')[1]\n",
        "      return sa_seq\n",
        "\n",
        "    # 9. Pair Single AA Sequences\n",
        "    elif data_type == \"A pair of AA Sequences\":\n",
        "      input_seq_1, input_seq_2 = raw_data\n",
        "      sa_seq1 = get_SA_sequence_by_data_type('Single AA Sequence', input_seq_1)\n",
        "      sa_seq2 = get_SA_sequence_by_data_type('Single AA Sequence', input_seq_2)\n",
        "\n",
        "      return (sa_seq1, sa_seq2)\n",
        "\n",
        "    # 10. Pair Single SA Sequences\n",
        "    elif data_type ==  \"A pair of SA Sequences\":\n",
        "      input_seq_1, input_seq_2 = raw_data\n",
        "      sa_seq1 = get_SA_sequence_by_data_type('Single SA Sequence', input_seq_1)\n",
        "      sa_seq2 = get_SA_sequence_by_data_type('Single SA Sequence', input_seq_2)\n",
        "\n",
        "      return (sa_seq1, sa_seq2)\n",
        "\n",
        "    # 11. Pair Single UniProt IDs\n",
        "    elif data_type ==  \"A pair of UniProt IDs\":\n",
        "      input_seq_1, input_seq_2 = raw_data\n",
        "      sa_seq1 = get_SA_sequence_by_data_type('Single UniProt ID', input_seq_1)\n",
        "      sa_seq2 = get_SA_sequence_by_data_type('Single UniProt ID', input_seq_2)\n",
        "\n",
        "      return (sa_seq1, sa_seq2)\n",
        "\n",
        "    # 12. Pair Single PDB/CIF Structure\n",
        "    elif data_type == \"A pair of PDB/CIF Structures\":\n",
        "      uniprot_id1 = raw_data[0]\n",
        "      struc_type1 = raw_data[1].value\n",
        "      chain1 = raw_data[2].value\n",
        "\n",
        "      protein_list1 = [(uniprot_id1, struc_type1, chain1)]\n",
        "      mprs1 = MultipleProcessRunnerSimplifier(protein_list1, pdb2sequence, n_process=2, return_results=True)\n",
        "      seqs1 = mprs1.run()\n",
        "      sa_seq1 = seqs1[0].split('\\t')[1]\n",
        "\n",
        "      uniprot_id2 = raw_data[3]\n",
        "      struc_type2 = raw_data[4].value\n",
        "      chain2 = raw_data[5].value\n",
        "\n",
        "      protein_list2 = [(uniprot_id2, struc_type2, chain2)]\n",
        "      mprs2 = MultipleProcessRunnerSimplifier(protein_list2, pdb2sequence, n_process=2, return_results=True)\n",
        "      seqs2 = mprs2.run()\n",
        "      sa_seq2 = seqs2[0].split('\\t')[1]\n",
        "      return sa_seq1, sa_seq2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################## Download predicted structures #######################\n",
        "################################################################################\n",
        "def uniprot2pdb(uniprot_ids, nprocess=20):\n",
        "  from saprot.utils.downloader import AlphaDBDownloader\n",
        "\n",
        "  os.makedirs(STRUCTURE_HOME, exist_ok=True)\n",
        "  af2_downloader = AlphaDBDownloader(uniprot_ids, \"pdb\", save_dir=STRUCTURE_HOME, n_process=20)\n",
        "  af2_downloader.run()\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############### Form foldseek sequences by multiple processes ##################\n",
        "################################################################################\n",
        "# def pdb2sequence(process_id, idx, uniprot_id, writer):\n",
        "#   from saprot.utils.foldseek_util import get_struc_seq\n",
        "\n",
        "#   try:\n",
        "#     pdb_path = f\"{STRUCTURE_HOME}/{uniprot_id}.pdb\"\n",
        "#     cif_path = f\"{STRUCTURE_HOME}/{uniprot_id}.cif\"\n",
        "#     if Path(pdb_path).exists():\n",
        "#       seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "#     if Path(cif_path).exists():\n",
        "#       seq = get_struc_seq(FOLDSEEK_PATH, cif_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "\n",
        "#     writer.write(f\"{uniprot_id}\\t{seq}\\n\")\n",
        "#   except Exception as e:\n",
        "#     print(f\"Error: {uniprot_id}, {e}\")\n",
        "\n",
        "# clear_output(wait=True)\n",
        "# print(\"Installation finished!\")\n",
        "\n",
        "def pdb2sequence(process_id, idx, row_tuple, writer):\n",
        "\n",
        "  # print(\"=\"*100)\n",
        "  # print(row_tuple)\n",
        "  # print(\"=\"*100)\n",
        "  uniprot_id = row_tuple[0].split('.')[0]     #\n",
        "  struc_type = row_tuple[1]                   # PDB or AF2\n",
        "  chain = row_tuple[2]\n",
        "\n",
        "  if struc_type==\"AF2\":\n",
        "    plddt_mask= True\n",
        "    chain = 'A'\n",
        "  else:\n",
        "    plddt_mask= False\n",
        "\n",
        "  from saprot.utils.foldseek_util import get_struc_seq\n",
        "\n",
        "  try:\n",
        "    pdb_path = f\"{STRUCTURE_HOME}/{uniprot_id}.pdb\"\n",
        "    cif_path = f\"{STRUCTURE_HOME}/{uniprot_id}.cif\"\n",
        "    if Path(pdb_path).exists():\n",
        "      seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, [chain], process_id=process_id, plddt_mask=plddt_mask)[chain][-1]\n",
        "    elif Path(cif_path).exists():\n",
        "      seq = get_struc_seq(FOLDSEEK_PATH, cif_path, [chain], process_id=process_id, plddt_mask=plddt_mask)[chain][-1]\n",
        "    else:\n",
        "      raise BaseException(f\"The {uniprot_id}.pdb/{uniprot_id}.cif file doesn't exists!\")\n",
        "    writer.write(f\"{uniprot_id}\\t{seq}\\n\")\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error: {uniprot_id}, {e}\")\n",
        "\n",
        "\n",
        "pymol_color_list = [\"#33ff33\",\"#00ffff\",\"#ff33cc\",\"#ffff00\",\"#ff9999\",\"#e5e5e5\",\"#7f7fff\",\"#ff7f00\",\n",
        "          \"#7fff7f\",\"#199999\",\"#ff007f\",\"#ffdd5e\",\"#8c3f99\",\"#b2b2b2\",\"#007fff\",\"#c4b200\",\n",
        "          \"#8cb266\",\"#00bfbf\",\"#b27f7f\",\"#fcd1a5\",\"#ff7f7f\",\"#ffbfdd\",\"#7fffff\",\"#ffff7f\",\n",
        "          \"#00ff7f\",\"#337fcc\",\"#d8337f\",\"#bfff3f\",\"#ff7fff\",\"#d8d8ff\",\"#3fffbf\",\"#b78c4c\",\n",
        "          \"#339933\",\"#66b2b2\",\"#ba8c84\",\"#84bf00\",\"#b24c66\",\"#7f7f7f\",\"#3f3fa5\",\"#a5512b\"]\n",
        "\n",
        "alphabet_list = list(ascii_uppercase+ascii_lowercase)\n",
        "\n",
        "\n",
        "def convert_outputs_to_pdb(outputs):\n",
        "\tfinal_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
        "\toutputs = {k: v.to(\"cpu\").numpy() for k, v in outputs.items()}\n",
        "\tfinal_atom_positions = final_atom_positions.cpu().numpy()\n",
        "\tfinal_atom_mask = outputs[\"atom37_atom_exists\"]\n",
        "\tpdbs = []\n",
        "\toutputs[\"plddt\"] *= 100\n",
        "\n",
        "\tfor i in range(outputs[\"aatype\"].shape[0]):\n",
        "\t\taa = outputs[\"aatype\"][i]\n",
        "\t\tpred_pos = final_atom_positions[i]\n",
        "\t\tmask = final_atom_mask[i]\n",
        "\t\tresid = outputs[\"residue_index\"][i] + 1\n",
        "\t\tpred = OFProtein(\n",
        "\t\t    aatype=aa,\n",
        "\t\t    atom_positions=pred_pos,\n",
        "\t\t    atom_mask=mask,\n",
        "\t\t    residue_index=resid,\n",
        "\t\t    b_factors=outputs[\"plddt\"][i],\n",
        "\t\t    chain_index=outputs[\"chain_index\"][i] if \"chain_index\" in outputs else None,\n",
        "\t\t)\n",
        "\t\tpdbs.append(to_pdb(pred))\n",
        "\treturn pdbs\n",
        "\n",
        "\n",
        "# This function is copied from ColabFold!\n",
        "def show_pdb(path, show_sidechains=False, show_mainchains=False, color=\"lddt\"):\n",
        "  file_type = str(path).split(\".\")[-1]\n",
        "  if file_type == \"cif\":\n",
        "    file_type == \"mmcif\"\n",
        "\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "  view.addModel(open(path,'r').read(),file_type)\n",
        "\n",
        "  if color == \"lDDT\":\n",
        "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "  elif color == \"rainbow\":\n",
        "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
        "  elif color == \"chain\":\n",
        "    chains = len(get_chain_ids(path))\n",
        "    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):\n",
        "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
        "\n",
        "  if show_sidechains:\n",
        "    BB = ['C','O','N']\n",
        "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "  if show_mainchains:\n",
        "    BB = ['C','O','N','CA']\n",
        "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "  view.zoomTo()\n",
        "  return view\n",
        "\n",
        "\n",
        "def plot_plddt_legend(dpi=100):\n",
        "  thresh = ['plDDT:','Very low (<50)','Low (60)','OK (70)','Confident (80)','Very high (>90)']\n",
        "  plt.figure(figsize=(1,0.1),dpi=dpi)\n",
        "  ########################################\n",
        "  for c in [\"#FFFFFF\",\"#FF0000\",\"#FFFF00\",\"#00FF00\",\"#00FFFF\",\"#0000FF\"]:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False,\n",
        "             loc='center', ncol=6,\n",
        "             handletextpad=1,\n",
        "             columnspacing=1,\n",
        "             markerscale=0.5,)\n",
        "  plt.axis(False)\n",
        "  return plt\n",
        "\n",
        "\n",
        "################################################################################\n",
        "###############   Download file to local computer   ##################\n",
        "################################################################################\n",
        "def file_download(path: str):\n",
        "  with open(path, \"rb\") as r:\n",
        "    res = r.read()\n",
        "\n",
        "  #FILE\n",
        "  filename = os.path.basename(path)\n",
        "  b64 = base64.b64encode(res)\n",
        "  payload = b64.decode()\n",
        "\n",
        "  #BUTTONS\n",
        "  html_buttons = '''<html>\n",
        "  <head>\n",
        "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "  </head>\n",
        "  <body>\n",
        "  <a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" download>\n",
        "  <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-warning\">Download File</button>\n",
        "  </a>\n",
        "  </body>\n",
        "  </html>\n",
        "  '''\n",
        "\n",
        "  html_button = html_buttons.format(payload=payload,filename=filename)\n",
        "  display(HTML(html_button))\n",
        "\n",
        "  # Automatically download file if the server is from google cloud.\n",
        "  if root_dir == \"/content\":\n",
        "    files.download(path)\n",
        "\n",
        "################################################################################\n",
        "############################ MODEL INFO #######################################\n",
        "################################################################################\n",
        "def get_base_model(adapter_path):\n",
        "  adapter_config = Path(adapter_path) / \"adapter_config.json\"\n",
        "  with open(adapter_config, 'r') as f:\n",
        "    adapter_config_dict = json.load(f)\n",
        "    base_model = adapter_config_dict['base_model_name_or_path']\n",
        "    if 'Protein_Encoder_650M' in base_model:\n",
        "      base_model = \"ProTrekHub/Protein_Encoder_650M\"\n",
        "    elif 'Protein_Encoder_35M' in base_model:\n",
        "      base_model = \"ProTrekHub/Protein_Encoder_35M\"\n",
        "    else:\n",
        "      raise RuntimeError(\"Please ensure the base model is \\\"Protein_650M\\\" or \\\"Protein_Encoder_35M\\\"\")\n",
        "  return base_model\n",
        "\n",
        "def check_training_data_type(adapter_path, data_type):\n",
        "  metadata_path = Path(adapter_path) / \"metadata.json\"\n",
        "  if metadata_path.exists():\n",
        "    with open(metadata_path, 'r') as f:\n",
        "      metadata = json.load(f)\n",
        "      required_training_data_type = metadata['training_data_type']\n",
        "  else:\n",
        "    required_training_data_type = \"SA\"\n",
        "\n",
        "  if (required_training_data_type == \"AA\") and (\"AA\" not in data_type):\n",
        "    print(Fore.RED+f\"This model ({adapter_path}) is trained on {required_training_data_type} sequences, and predictions work better with AA sequences.\"+Style.RESET_ALL)\n",
        "    print(Fore.RED+f\"The current data type ({data_type}) includes structural information, which will not be used for predictions.\"+Style.RESET_ALL)\n",
        "    print()\n",
        "    print('='*100)\n",
        "  elif (required_training_data_type == \"SA\") and (\"AA\" in data_type):\n",
        "    print(Fore.RED+f\"This model ({adapter_path}) is trained on {required_training_data_type} sequences, and predictions work better with SA sequences.\"+Style.RESET_ALL)\n",
        "    print(Fore.RED+f\"The current data type ({data_type}) does not include structural information, which may lead to weak prediction performance.\"+Style.RESET_ALL)\n",
        "    print(Fore.RED+f\"If you only have the amino acid sequence, we strongly recommend using AF2 to predict the structure and generate a PDB file before prediction.\"+Style.RESET_ALL)\n",
        "    print()\n",
        "    print('='*100)\n",
        "\n",
        "  return required_training_data_type\n",
        "\n",
        "def mask_struc_token(sequence):\n",
        "    return ''.join('#' if i % 2 == 1 and char.islower() else char for i, char in enumerate(sequence))\n",
        "\n",
        "def get_num_labels_by_adapter(adapter_path):\n",
        "    adapter_path = Path(adapter_path)\n",
        "\n",
        "    if (adapter_path / 'adapter_model.safetensors').exists():\n",
        "        file_path = adapter_path / 'adapter_model.safetensors'\n",
        "        with safe_open(file_path, framework=\"pt\") as f:\n",
        "          if 'base_model.model.classifier.out_proj.bias' in f.keys():\n",
        "              tensor = f.get_tensor('base_model.model.classifier.out_proj.bias')\n",
        "          elif 'base_model.model.classifier.bias' in f.keys():\n",
        "              tensor = f.get_tensor('base_model.model.classifier.bias')\n",
        "          else:\n",
        "              raise KeyError(f\"Neither 'base_model.model.classifier.out_proj.bias' nor 'base_model.model.classifier.bias' found in the file({file_path}).\")\n",
        "\n",
        "    elif (adapter_path / 'adapter_model.bin').exists():\n",
        "      file_path = adapter_path / 'adapter_model.bin'\n",
        "      state_dict = torch.load(file_path)\n",
        "      if 'base_model.model.classifier.out_proj.bias' in state_dict.keys():\n",
        "        tensor = state_dict['base_model.model.classifier.out_proj.bias']\n",
        "      elif 'base_model.model.classifier.bias' in f.keys():\n",
        "        tensor = state_dict['base_model.model.classifier.bias']\n",
        "      else:\n",
        "        raise KeyError(f\"Neither 'base_model.model.classifier.out_proj.bias' nor 'base_model.model.classifier.bias' found in the file({file_path}).\")\n",
        "\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Neither 'adapter_model.safetensors' nor 'adapter_model.bin' found in the provided path({adapter_path}).\")\n",
        "\n",
        "    num_labels = list(tensor.shape)[0]\n",
        "    return num_labels\n",
        "\n",
        "def get_num_labels_and_task_type_by_adapter(adapter_path):\n",
        "    adapter_path = Path(adapter_path)\n",
        "\n",
        "    task_type = None\n",
        "    if (adapter_path / 'adapter_model.safetensors').exists():\n",
        "      file_path = adapter_path / 'adapter_model.safetensors'\n",
        "      with safe_open(file_path, framework=\"pt\") as f:\n",
        "        if 'base_model.model.classifier.out_proj.bias' in f.keys():\n",
        "          tensor = f.get_tensor('base_model.model.classifier.out_proj.bias')\n",
        "        elif 'base_model.model.classifier.bias' in f.keys():\n",
        "          task_type = 'token_classification'\n",
        "          tensor = f.get_tensor('base_model.model.classifier.bias')\n",
        "        else:\n",
        "          raise KeyError(f\"Neither 'base_model.model.classifier.out_proj.bias' nor 'base_model.model.classifier.bias' found in the file({file_path}).\")\n",
        "\n",
        "    elif (adapter_path / 'adapter_model.bin').exists():\n",
        "      file_path = adapter_path / 'adapter_model.bin'\n",
        "      state_dict = torch.load(file_path)\n",
        "      if 'base_model.model.classifier.out_proj.bias' in state_dict.keys():\n",
        "        tensor = state_dict['base_model.model.classifier.out_proj.bias']\n",
        "      elif 'base_model.model.classifier.bias' in f.keys():\n",
        "        task_type = 'token_classification'\n",
        "        tensor = state_dict['base_model.model.classifier.bias']\n",
        "      else:\n",
        "        raise KeyError(f\"Neither 'base_model.model.classifier.out_proj.bias' nor 'base_model.model.classifier.bias' found in the file({file_path}).\")\n",
        "\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Neither 'adapter_model.safetensors' nor 'adapter_model.bin' found in the provided path({adapter_path}).\")\n",
        "\n",
        "    num_labels = list(tensor.shape)[0]\n",
        "    if task_type != 'token_classification':\n",
        "      if num_labels > 1:\n",
        "        task_type = 'classification'\n",
        "      elif num_labels == 1:\n",
        "        task_type = 'regression'\n",
        "\n",
        "    return num_labels, task_type\n",
        "\n",
        "clear_output(wait=True)\n",
        "print(\"Installation finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uxag_RSBI7e"
      },
      "source": [
        "# **2: Train and Share your Protein Model** <a name=\"train\"></a>\n",
        "\n",
        "You can **train** a model based on pre-trained ProTrek, or **continually train** a fine-tuned model in ProTrekHub.\n",
        "\n",
        "\n",
        "\n",
        "<!-- ## Training Dataset\n",
        "\n",
        "For the training dataset, **two additional columns** are required in the CSV file: `label` and `stage`.\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/Multiple_AA_Sequences_data_format_training.png\n",
        "?raw=true\" height=\"200\" width=\"400px\" align=\"center\">\n",
        "\n",
        "### Column `label`\n",
        "\n",
        "The content of column `label` depends on your **task type**:\n",
        "\n",
        "| Task Type                         | Content in the Column                          |\n",
        "|-----------------------------------|------------------------------------------------|\n",
        "| Classification tasks              | Category index starting from zero              |\n",
        "| Amino Acid Classification tasks   | A list of category indices for each amino acid |\n",
        "| Regression tasks                  | Numerical values                               |\n",
        "\n",
        "<img src=\"https://github.com/westlake-repl/SaProtHub/blob/main/Figure/label_format.png?raw=true\" height=\"300\" width=\"800px\" align=\"center\">\n",
        "<br>\n",
        "\n",
        "\n",
        "### Column `stage`\n",
        "\n",
        "The column `stage` indicate whether the sample is used for training, validation, or testing. Ensure your dataset includes samples for all three stages. The values are: `train`, `valid`, `test`.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Note:**\n",
        "\n",
        "1. **Examples are available** at /content/SaprotHub/upload_files (if you connect to your local server, then the path is /SaprotHub/upload_files). Download to review their format, and then upload them for a trial.\n",
        "\n",
        "2.  <a href=\"#get_sa\">Here</a> you can **convert your data into SA Sequence** format.\n",
        "\n",
        "3. <a href=\"#fa2csv\">Here</a> you can **convert your .fa/.fasta file to a .csv file**, which corresponds to the data format for Multiple AA Sequences.\n",
        "\n",
        "4. <a href=\"#split_dataset\">Here</a> you can **randomly split your .csv dataset**, which means to add a `stage` column, where the ratio of `train`:`valid`:`test` is 8:1:1.\n",
        "\n",
        "4. The maximum input length of the model is 1024, and protein sequences exceeding this length will only retain the first 1024 amino acids. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vqdmLslQBI7e"
      },
      "outputs": [],
      "source": [
        "#@title **2.1: Train your Model** <a name=\"train\"></a>\n",
        "\n",
        "################################################################################\n",
        "############################# ADVANCED CONFIG ##################################\n",
        "################################################################################\n",
        "\n",
        "# training config\n",
        "GPU_batch_size = 0\n",
        "accumulate_grad_batches = 0\n",
        "num_workers = 2\n",
        "seed = 20000812\n",
        "\n",
        "# lora config\n",
        "r = 8\n",
        "lora_dropout = 0.0\n",
        "lora_alpha = 16\n",
        "\n",
        "# dataset config\n",
        "val_check_interval=0.5\n",
        "limit_train_batches=1.0\n",
        "limit_val_batches=1.0\n",
        "limit_test_batches=1.0\n",
        "\n",
        "\n",
        "mask_struc_ratio=None\n",
        "\n",
        "################################################################################\n",
        "################################## MARKDOWN #################################\n",
        "################################################################################\n",
        "#@markdown ‚ö†Ô∏èIf you want to **interrupt** the training, **do not** click the run button again. Please refer to [here](https://github.com/westlake-repl/SaprotHub/wiki/ColabProTrek-&-ColabProtT5#interrupt-training-to-avoid-overfitting).\n",
        "\n",
        "#@markdown > üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/ColabProTrek-&-ColabProtT5#21-Train-your-model)\n",
        "\n",
        "if torch.cuda.is_available() is False:\n",
        "  raise BaseException(\"Please refer to Section 1.1 to switch your Runtime to a GPU!\")\n",
        "\n",
        "################################################################################\n",
        "################################## TASK CONFIG #################################\n",
        "################################################################################\n",
        "#@markdown # 1. Task\n",
        "task_name = \"demo\" # @param {type:\"string\"}\n",
        "task_type = \"Protein-level Regression\" # @param [\"Protein-level Classification\", \"Protein-level Regression\", \"Residue-level Classification\", \"Protein-protein Classification\", \"Protein-protein Regression\"]\n",
        "original_task_type = task_type\n",
        "task_type = task_type_dict[task_type]\n",
        "\n",
        "if task_type in [\"classification\", 'token_classification', 'pair_classification']:\n",
        "\n",
        "  print(Fore.BLUE+'Enter the number of category in your training dataset here:'+Style.RESET_ALL)\n",
        "  num_of_categories = ipywidgets.BoundedIntText(\n",
        "                                              # value=7,\n",
        "                                              min=2,\n",
        "                                              max=1000000,\n",
        "                                              step=1,\n",
        "                                              # description='num_of_category: \\n',\n",
        "                                              disabled=False)\n",
        "  num_of_categories.layout.width = \"100px\"\n",
        "  display(num_of_categories)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "#################################### MODEL CONFIG #####################################\n",
        "################################################################################\n",
        "#@markdown # 2. Model\n",
        "\n",
        "base_model = \"Official pretrained ProTrek (35M)\" # @param [\"Official pretrained ProTrek (35M)\", \"Official pretrained ProTrek (650M)\", \"Trained by yourself on ColabProTrek\", \"Shared by peers on ProTrekHub\", \"Saved in your local computer\"]\n",
        "\n",
        "# continue learning\n",
        "if base_model in [\"Trained by yourself on ColabProTrek\", \"Shared by peers on ProTrekHub\", \"Saved in your local computer\"]:\n",
        "  continue_learning = True\n",
        "  adapter_combobox = select_adapter_from(task_type, use_model_from=base_model)\n",
        "else:\n",
        "  continue_learning = False\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################### DATASET CONFIG ####################################\n",
        "################################################################################\n",
        "#@markdown # 3. Dataset\n",
        "\n",
        "data_type = \"Multiple AA Sequences\" # @param [\"SaprotHub Dataset\",\"Multiple AA Sequences\",\"Multiple UniProt IDs\",\"Multiple pairs of AA Sequences\",\"Multiple pairs of UniProt IDs\"]\n",
        "check_task_type_and_data_type(original_task_type, data_type)\n",
        "\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################### TRAIN CONFIG ####################################\n",
        "################################################################################\n",
        "#@markdown # 4. Training\n",
        "\n",
        "batch_size = \"Adaptive\" # @param [\"Adaptive\", \"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"256\"]\n",
        "max_epochs = 10 # @param [\"10\", \"20\", \"50\"] {type:\"raw\", allow-input: true}\n",
        "learning_rate = 1.0e-4 # @param [\"1.0e-3\", \"5.0e-4\", \"1.0e-4\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################# CONFIG #######################################\n",
        "################################################################################\n",
        "\n",
        "from saprot.config.config_dict import Default_config\n",
        "config = copy.deepcopy(Default_config)\n",
        "\n",
        "################################################################################\n",
        "################################### TRAIN ####################################\n",
        "################################################################################\n",
        "\n",
        "def train(button):\n",
        "  global base_model\n",
        "  global GPU_batch_size\n",
        "  global accumulate_grad_batches\n",
        "\n",
        "  button.disabled = True\n",
        "  button.description = 'Training...'\n",
        "  button.button_style = ''\n",
        "\n",
        "################################################################################\n",
        "################################### DATASET CONFIRM ####################################\n",
        "################################################################################\n",
        "  csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "  new_path = check_column_label_and_stage(csv_dataset_path)\n",
        "  from saprot.utils.construct_lmdb import construct_lmdb\n",
        "  construct_lmdb(new_path, LMDB_HOME, task_name, task_type)\n",
        "  lmdb_dataset_path = LMDB_HOME / task_name\n",
        "\n",
        "################################################################################\n",
        "################################### MODEL CONFIRM ####################################\n",
        "################################################################################\n",
        "\n",
        "  # base_model\n",
        "  if continue_learning:\n",
        "    adapter_path = ADAPTER_HOME / task_type / adapter_combobox.value\n",
        "    print(f\"Training on an existing model: {adapter_path}\")\n",
        "\n",
        "    if base_model == \"Shared by peers on ProTrekHub\":\n",
        "      if not adapter_path.exists():\n",
        "        snapshot_download(repo_id=adapter_combobox.value, repo_type=\"model\", local_dir=adapter_path)\n",
        "\n",
        "    adapter_config_path = Path(adapter_path) / \"adapter_config.json\"\n",
        "    assert adapter_config_path.exists(), f\"Can't find {adapter_config_path}\"\n",
        "    with open(adapter_config_path, 'r') as f:\n",
        "      adapter_config = json.load(f)\n",
        "      base_model = adapter_config['base_model_name_or_path']\n",
        "\n",
        "  elif base_model == \"Official pretrained ProTrek (35M)\":\n",
        "    base_model = \"ProTrekHub/Protein_Encoder_35M\"\n",
        "\n",
        "  elif base_model == \"Official pretrained ProTrek (650M)\":\n",
        "    base_model = \"ProTrekHub/Protein_Encoder_650M\"\n",
        "\n",
        "  elif base_model == \"Official pretrained ProtT5 (2.8B)\":\n",
        "    base_model = \"Rostlab/prot_t5_xl_uniref50\"\n",
        "\n",
        "  # model size and model name\n",
        "  if base_model == \"ProTrekHub/Protein_Encoder_650M\":\n",
        "    model_size = \"650M\"\n",
        "    model_name = f\"Model-{task_name}-{model_size}\"\n",
        "  elif base_model == \"ProTrekHub/Protein_Encoder_35M\":\n",
        "    model_size = \"35M\"\n",
        "    model_name = f\"Model-{task_name}-{model_size}\"\n",
        "  elif base_model == \"Rostlab/prot_t5_xl_uniref50\":\n",
        "    model_size = \"2.8B\"\n",
        "    model_name = f\"Model-{task_name}-{model_size}\"\n",
        "\n",
        "  config.setting.run_mode = \"train\"\n",
        "  config.setting.seed = seed\n",
        "\n",
        "################################################################################\n",
        "################################# MODEL ########################################\n",
        "################################################################################\n",
        "\n",
        "  if task_type in [\"classification\", \"token_classification\", \"pair_classification\"]:\n",
        "    config.model.kwargs.num_labels = num_of_categories.value\n",
        "\n",
        "  config.model.model_py_path = model_type_dict[task_type]\n",
        "  config.model.kwargs.config_path = base_model\n",
        "  config.dataset.kwargs.tokenizer = base_model\n",
        "\n",
        "  config.model.save_path = str(ADAPTER_HOME / f\"{task_type}\" / \"Local\" / model_name)\n",
        "\n",
        "  if task_type in [\"regression\", \"pair_regression\"]:\n",
        "    config.model.kwargs.extra_config = {}\n",
        "    config.model.kwargs.extra_config.attention_probs_dropout_prob=0\n",
        "    config.model.kwargs.extra_config.hidden_dropout_prob=0\n",
        "\n",
        "  config.model.kwargs.lora_kwargs = EasyDict({\n",
        "    \"is_trainable\": True,\n",
        "    \"num_lora\": 1,\n",
        "    \"r\": r,\n",
        "    \"lora_dropout\": lora_dropout,\n",
        "    \"lora_alpha\": lora_alpha,\n",
        "    \"config_list\": []})\n",
        "  if continue_learning:\n",
        "    config.model.kwargs.lora_kwargs.config_list.append({\"lora_config_path\": adapter_path})\n",
        "\n",
        "################################################################################\n",
        "################################# DATASET ######################################\n",
        "################################################################################\n",
        "\n",
        "  config.dataset.dataset_py_path = dataset_type_dict[task_type]\n",
        "\n",
        "  config.dataset.train_lmdb = str(lmdb_dataset_path / \"train\")\n",
        "  config.dataset.valid_lmdb = str(lmdb_dataset_path / \"valid\")\n",
        "  config.dataset.test_lmdb = str(lmdb_dataset_path / \"test\")\n",
        "\n",
        "  # num_workers\n",
        "  config.dataset.dataloader_kwargs.num_workers = num_workers\n",
        "\n",
        "  # mask_struc\n",
        "  # config.dataset.kwargs.mask_struc_ratio= mask_struc_ratio\n",
        "\n",
        "  ################################################################################\n",
        "  ######################## batch size ############################################\n",
        "  ################################################################################\n",
        "  def get_accumulate_grad_samples(num_samples):\n",
        "      if num_samples > 3200:\n",
        "          return 64\n",
        "      elif 1600 < num_samples <= 3200:\n",
        "          return 32\n",
        "      elif 800 < num_samples <= 1600:\n",
        "          return 16\n",
        "      elif 400 < num_samples <= 800:\n",
        "          return 8\n",
        "      elif 200 < num_samples <= 400:\n",
        "          return 4\n",
        "      elif 100 < num_samples <= 200:\n",
        "          return 2\n",
        "      else:\n",
        "          return 1\n",
        "\n",
        "  # advanced config\n",
        "  if (GPU_batch_size > 0) and (accumulate_grad_batches > 0):\n",
        "    config.dataset.dataloader_kwargs.batch_size = GPU_batch_size\n",
        "    config.Trainer.accumulate_grad_batches= accumulate_grad_batches\n",
        "\n",
        "  elif (GPU_batch_size == 0) and (accumulate_grad_batches == 0):\n",
        "\n",
        "    # batch_size\n",
        "    if base_model == \"westlake-repl/ProTrek_650M_UniRef50\" and root_dir == \"/content\":\n",
        "      GPU_batch_size = 1\n",
        "    else:\n",
        "      GPU_batch_size_dict = {\n",
        "        \"Tesla T4\": 2,\n",
        "        \"NVIDIA L4\": 2,\n",
        "        \"NVIDIA A100-SXM4-40GB\": 4,\n",
        "        }\n",
        "      GPU_name = torch.cuda.get_device_name(0)\n",
        "      GPU_batch_size = GPU_batch_size_dict[GPU_name] if GPU_name in GPU_batch_size_dict else 2\n",
        "\n",
        "      if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "        GPU_batch_size = int(max(GPU_batch_size / 2, 1))\n",
        "\n",
        "    config.dataset.dataloader_kwargs.batch_size = GPU_batch_size\n",
        "\n",
        "    # accumulate_grad_batches\n",
        "    if batch_size == \"Adaptive\":\n",
        "\n",
        "      env = lmdb.open(config.dataset.train_lmdb, readonly=True)\n",
        "\n",
        "      with env.begin() as txn:\n",
        "        stat = txn.stat()\n",
        "        num_samples = stat['entries']\n",
        "\n",
        "      accumulate_grad_samples = get_accumulate_grad_samples(num_samples)\n",
        "\n",
        "    else:\n",
        "      accumulate_grad_samples = int(batch_size)\n",
        "\n",
        "    accumulate_grad_batches = max(int(accumulate_grad_samples / GPU_batch_size), 1)\n",
        "\n",
        "    config.Trainer.accumulate_grad_batches= accumulate_grad_batches\n",
        "\n",
        "  else:\n",
        "    raise BaseException(f\"Please make sure `GPU_batch_size`({GPU_batch_size}) and `accumulate_grad_batches`({accumulate_grad_batches}) are both greater than zero!\")\n",
        "\n",
        "  ################################################################################\n",
        "  ############################## TRAINER #########################################\n",
        "  ################################################################################\n",
        "\n",
        "  config.Trainer.accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  # epoch\n",
        "  config.Trainer.max_epochs = max_epochs\n",
        "  # test only: load the existing model\n",
        "  if config.Trainer.max_epochs == 0 and continue_learning:\n",
        "    config.model.save_path = config.model.kwargs.lora_kwargs.config_list[0]['lora_config_path']\n",
        "\n",
        "  # learning rate\n",
        "  config.model.lr_scheduler_kwargs.init_lr = learning_rate\n",
        "\n",
        "  # trainer\n",
        "  config.Trainer.limit_train_batches=limit_train_batches\n",
        "  config.Trainer.limit_val_batches=limit_val_batches\n",
        "  config.Trainer.limit_test_batches=limit_test_batches\n",
        "  config.Trainer.val_check_interval=val_check_interval\n",
        "\n",
        "  # strategy\n",
        "  strategy = {\n",
        "      # - deepspeed\n",
        "      # 'class': 'DeepSpeedStrategy',\n",
        "      # 'stage': 2\n",
        "\n",
        "      # - None\n",
        "      # 'class': None,\n",
        "\n",
        "      # - DP\n",
        "      # 'class': 'DataParallelStrategy',\n",
        "\n",
        "      # - DDP\n",
        "      # 'class': 'DDPStrategy',\n",
        "      # 'find_unused_parameter': True\n",
        "  }\n",
        "  config.Trainer.strategy = strategy\n",
        "\n",
        "  ################################################################################\n",
        "  ############################## Run the task ####################################\n",
        "  ################################################################################\n",
        "\n",
        "  print('='*100)\n",
        "  print(Fore.BLUE+f\"Training task type: {task_type}\"+Style.RESET_ALL)\n",
        "  print(Fore.BLUE+f\"Dataset: {lmdb_dataset_path}\"+Style.RESET_ALL)\n",
        "  print(Fore.BLUE+f\"Base Model: {config.model.kwargs.config_path}\"+Style.RESET_ALL)\n",
        "  if continue_learning:\n",
        "    print(Fore.BLUE+f\"Existing model: {config.model.kwargs.lora_kwargs.config_list[0]['lora_config_path']}\"+Style.RESET_ALL)\n",
        "  print('='*100)\n",
        "  pprint.pprint(config)\n",
        "  print('='*100)\n",
        "\n",
        "  from saprot.scripts.training import finetune\n",
        "  finetune(config)\n",
        "\n",
        "\n",
        "  ################################################################################\n",
        "  ############################## Save the adapter ################################\n",
        "  ################################################################################\n",
        "\n",
        "  def add_training_data_type_to_config(metadata_path, training_data_type):\n",
        "    if metadata_path.exists() is False:\n",
        "      config_data = {\n",
        "          'training_data_type': training_data_type\n",
        "          }\n",
        "      with open(metadata_path, 'w') as file:\n",
        "          json.dump(config_data, file, indent=4)\n",
        "\n",
        "    else:\n",
        "      with open(metadata_path, 'r') as file:\n",
        "          config_data = json.load(file)\n",
        "\n",
        "      config_data['training_data_type'] = training_data_type\n",
        "\n",
        "      with open(metadata_path, 'w') as file:\n",
        "          json.dump(config_data, file, indent=4)\n",
        "\n",
        "  metadata_path = Path(config.model.save_path) / \"metadata.json\"\n",
        "  training_data_type = training_data_type_dict[data_type]\n",
        "  add_training_data_type_to_config(metadata_path, training_data_type)\n",
        "\n",
        "  print(Fore.BLUE)\n",
        "  print(f\"Model is saved to \\\"{config.model.save_path}\\\" on Colab Server\")\n",
        "  print(Style.RESET_ALL)\n",
        "\n",
        "\n",
        "  adapter_zip = Path(config.model.save_path) / f\"{model_name}.zip\"\n",
        "  !cd $config.model.save_path && zip -r $adapter_zip \"adapter_config.json\" \"adapter_model.safetensors\" \"README.md\" \"metadata.json\"\n",
        "  # !cd $config.model.save_path && zip -r $adapter_zip \"adapter_config.json\" \"adapter_model.safetensors\" \"adapter_model.bin\" \"README.md\" \"metadata.json\"\n",
        "  print(\"Click to download the model to your local computer\")\n",
        "  if adapter_zip.exists():\n",
        "    # files.download(adapter_zip)\n",
        "    file_download(adapter_zip)\n",
        "\n",
        "\n",
        "\n",
        "  ################################################################################\n",
        "  ############################### Modify README ##################################\n",
        "  ################################################################################\n",
        "  name = model_name\n",
        "  description = '<slot name=\\'description\\'>'\n",
        "  label_meanings = '<slot name=\\'label_meanings\\'>'\n",
        "\n",
        "  with open(f'{config.model.save_path}/adapter_config.json', 'r') as f:\n",
        "    lora_config = json.load(f)\n",
        "\n",
        "  markdown = f'''\n",
        "---\n",
        "\n",
        "base_model: {base_model} \\n\n",
        "library_name: peft\n",
        "\n",
        "---\n",
        "\\n\n",
        "\n",
        "# Model Card for {name}\n",
        "{description}\n",
        "\n",
        "## Task type\n",
        "{original_task_type}\n",
        "\n",
        "## Model input type\n",
        "{training_data_type_dict[data_type]} Sequence\n",
        "\n",
        "## Label meanings\n",
        "{label_meanings}\n",
        "\n",
        "## LoRA config\n",
        "\n",
        "- **r:** {lora_config['r']}\n",
        "- **lora_dropout:** {lora_config['lora_dropout']}\n",
        "- **lora_alpha:** {lora_config['lora_alpha']}\n",
        "- **target_modules:** {lora_config['target_modules']}\n",
        "- **modules_to_save:** {lora_config['modules_to_save']}\n",
        "\n",
        "## Training config\n",
        "\n",
        "- **optimizer:**\n",
        "  - **class:** AdamW\n",
        "  - **betas:** (0.9, 0.98)\n",
        "  - **weight_decay:** 0.01\n",
        "- **learning rate:** {config.model.lr_scheduler_kwargs.init_lr}\n",
        "- **epoch:** {config.Trainer.max_epochs}\n",
        "- **batch size:** {config.dataset.dataloader_kwargs.batch_size * config.Trainer.accumulate_grad_batches}\n",
        "- **precision:** 16-mixed \\n\n",
        "'''\n",
        "\n",
        "  # Write the markdown output to a file\n",
        "  with open(f\"{config.model.save_path}/README.md\", \"w\") as file:\n",
        "    file.write(markdown)\n",
        "\n",
        "\n",
        "button_train = ipywidgets.Button(\n",
        "    description='Start Training',\n",
        "    disabled=False,\n",
        "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Apply',\n",
        "    # icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "    )\n",
        "button_train.on_click(train)\n",
        "button_train.layout.width = '300px'\n",
        "display(button_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UnKX1BTZBI7f"
      },
      "outputs": [],
      "source": [
        "#@title **2.2: Login HuggingFace to Upload your model (Optional)** <a name=\"upload_model\"></a>\n",
        "################################################################################\n",
        "###################### Login HuggingFace #######################################\n",
        "################################################################################\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6XlluTsPBI7m"
      },
      "outputs": [],
      "source": [
        "#@title **2.3: Upload your Model (Optional)**\n",
        "\n",
        "#@markdown > üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/ColabProTrek-&-ColabProtT5#22-Upload-your-model)\n",
        "\n",
        "# #@markdown Your Huggingface adapter repository names follow the format `<username>/<task_name>`.\n",
        "\n",
        "################################################################################\n",
        "########################## Metadata  ###########################################\n",
        "################################################################################\n",
        "name = \"demo_cls\" # @param {type:\"string\"}\n",
        "description = \"This model is used for a demo classification task\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "# #@markdown > 0: Nucleus <br>\n",
        "# #@markdown > 1: Cytoplasm <br>\n",
        "# #@markdown > 2: Extracellular <br>\n",
        "# #@markdown > ... <br>\n",
        "# #@markdown > 9: Peroxisome <br>\n",
        "\n",
        "label_meanings = \"A, B\" #@param {type:\"string\"}\n",
        "\n",
        "################################################################################\n",
        "########################### Move Files  ########################################\n",
        "################################################################################\n",
        "\n",
        "from huggingface_hub import HfApi, Repository, ModelFilter\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "user = api.whoami()\n",
        "\n",
        "if name == \"\":\n",
        "  name = model_name\n",
        "repo_name = user['name'] + '/' + name\n",
        "local_dir = Path(\"/content/SaprotHub/model_to_push\") / repo_name\n",
        "local_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "repo_list = [repo.id for repo in api.list_models(filter=ModelFilter(author=user['name']))]\n",
        "if repo_name not in repo_list:\n",
        "  api.create_repo(repo_name, private=False)\n",
        "\n",
        "repo = Repository(local_dir=local_dir, clone_from=repo_name)\n",
        "\n",
        "command = f\"cp {config.model.save_path}/* {local_dir}/\"\n",
        "subprocess.run(command, shell=True)\n",
        "\n",
        "################################################################################\n",
        "########################## Modify README  ######################################\n",
        "################################################################################\n",
        "import json\n",
        "\n",
        "md_path = local_dir / \"README.md\"\n",
        "\n",
        "\n",
        "if task_type in [\"classification\", \"token_classification\", \"pair_classification\"]:\n",
        "    label_meanings_md = ''\n",
        "    for index, label in enumerate(label_meanings.split(', ')):\n",
        "      label_meanings_md += f'''\n",
        "{index}: {label.strip()}\n",
        "'''\n",
        "    label_meanings = label_meanings_md\n",
        "\n",
        "replace_data = {\n",
        "    '<slot name=\\'description\\'>': description,\n",
        "    '<slot name=\\'label_meanings\\'>': label_meanings\n",
        "}\n",
        "\n",
        "with open(md_path, \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "for key, value in replace_data.items():\n",
        "    if value != \"\":\n",
        "        content = content.replace(key, value)\n",
        "\n",
        "with open(md_path, \"w\") as file:\n",
        "    file.write(content)\n",
        "\n",
        "################################################################################\n",
        "########################## Upload Model  #######################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "repo.push_to_hub(commit_message=\"Upload adapter model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ1JgmrsBI7m"
      },
      "source": [
        "# **3: Use ProTrek to Predict**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "h8qHRJtIQxU4"
      },
      "outputs": [],
      "source": [
        "#@title **3.1: Classification&Regression Prediction** <a name=\"prediction\"></a>\n",
        "\n",
        "#@markdown > üìçPlease see the [tutorial](https://github.com/westlake-repl/SaprotHub/wiki/ColabProTrek-&-ColabProtT5#31-Classification-Regression-Prediction)\n",
        "\n",
        "from transformers import EsmTokenizer\n",
        "import torch\n",
        "import copy\n",
        "import sys\n",
        "from saprot.scripts.training import my_load_model\n",
        "\n",
        "################################################################################\n",
        "################################# TASK #########################################\n",
        "################################################################################\n",
        "#@markdown # 1. Task\n",
        "\n",
        "task_type = \"Protein-level Regression\" # @param [\"Protein-level Classification\", \"Protein-level Regression\", \"Residue-level Classification\", \"Protein-protein Classification\", \"Protein-protein Regression\"]\n",
        "original_task_type = task_type\n",
        "task_type = task_type_dict[task_type]\n",
        "\n",
        "if task_type in [\"classification\", 'token_classification', 'pair_classification']:\n",
        "\n",
        "  print(Fore.BLUE+'The number of categories in your classification task:'+Style.RESET_ALL)\n",
        "  num_of_categories = ipywidgets.BoundedIntText(\n",
        "                                              # value=7,\n",
        "                                              min=2,\n",
        "                                              # max=10,\n",
        "                                              step=1,\n",
        "                                              # description='num_of_category: \\n',\n",
        "                                              disabled=False)\n",
        "  num_of_categories.layout.width = \"100px\"\n",
        "  display(num_of_categories)\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################## MODEL #######################################\n",
        "################################################################################\n",
        "#@markdown # 2. Model\n",
        "\n",
        "use_model_from = \"Trained by yourself on ColabProTrek\" # @param [\"Trained by yourself on ColabProTrek\", \"Shared by peers on ProTrekHub\", \"Saved in your local computer\", \"Multi-models on ProTrekHub\"]\n",
        "if use_model_from == \"Multi-models on ProTrekHub\":\n",
        "  multi_lora = True\n",
        "else:\n",
        "  multi_lora = False\n",
        "\n",
        "adapter_input = select_adapter_from(task_type, use_model_from)\n",
        "#@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "################################ DATASET #######################################\n",
        "################################################################################\n",
        "#@markdown # 3. Dataset\n",
        "data_type = \"Single AA Sequence\" # @param [\"Single AA Sequence\",\"Single UniProt ID\",\"Multiple AA Sequences\",\"Multiple UniProt IDs\",\"A pair of AA Sequences\",\"A pair of UniProt IDs\",\"Multiple pairs of AA Sequences\",\"Multiple pairs of UniProt IDs\"]\n",
        "check_task_type_and_data_type(original_task_type, data_type)\n",
        "\n",
        "mode = \"Multiple Sequences\" if (data_type in data_type_list_multiple) else \"Single Sequence\"\n",
        "\n",
        "raw_data = input_raw_data_by_data_type(data_type)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "##################################### PREDICT ###################################\n",
        "################################################################################\n",
        "def predict(button):\n",
        "  button.disabled = True\n",
        "  button.description = 'Predicting...'\n",
        "  button.button_style = ''\n",
        "\n",
        "  print('\\n')\n",
        "  print('='*100)\n",
        "\n",
        "  ##############################################################################\n",
        "  ################################# MODEL ###################################\n",
        "  ##############################################################################\n",
        "  if multi_lora:\n",
        "    if use_model_from == \"Multi-models on ColabProTrek\":\n",
        "      config_list = [EasyDict({'lora_config_path': ADAPTER_HOME / task_type / lora_config_path}) for lora_config_path in list(adapter_input.value)]\n",
        "    elif use_model_from == \"Multi-models on ProTrekHub\":\n",
        "      #1. get adapter_list\n",
        "      repo_id_list = adapter_input.value.replace(\" \", \"\").split(',')\n",
        "      #2. download adapters\n",
        "      for repo_id in repo_id_list:\n",
        "        snapshot_download(repo_id=repo_id, repo_type=\"model\", local_dir=ADAPTER_HOME / task_type / repo_id)\n",
        "      config_list = [EasyDict({'lora_config_path': ADAPTER_HOME / task_type / repo_id}) for repo_id in repo_id_list]\n",
        "\n",
        "    assert len(config_list) > 0, \"Please select your models from the dropdown menu on the output of 3.1!\"\n",
        "    base_model = get_base_model(ADAPTER_HOME / task_type / config_list[0].lora_config_path)\n",
        "\n",
        "    required_training_data_type_list = []\n",
        "    for lora_config in config_list:\n",
        "      required_training_data_type_list.append(check_training_data_type(lora_config.lora_config_path, data_type))\n",
        "    # assert len(set(required_training_data_type_list)) == 1, f\"Error: The input data types of these models are not identical: {required_training_data_type_list}\"\n",
        "    required_training_data_type = required_training_data_type_list[0]\n",
        "\n",
        "    lora_kwargs = EasyDict({\n",
        "      \"is_trainable\": False,\n",
        "      \"num_lora\": len(config_list),\n",
        "      \"config_list\": config_list\n",
        "    })\n",
        "\n",
        "  else:\n",
        "    if use_model_from == \"Shared by peers on ProTrekHub\":\n",
        "      snapshot_download(repo_id=adapter_input.value, repo_type=\"model\", local_dir=ADAPTER_HOME / task_type / adapter_input.value)\n",
        "\n",
        "    adapter_path = ADAPTER_HOME / task_type / adapter_input.value\n",
        "    base_model = get_base_model(adapter_path)\n",
        "    required_training_data_type = check_training_data_type(adapter_path, data_type)\n",
        "    lora_kwargs = {\n",
        "      \"is_trainable\": False,\n",
        "      \"num_lora\": 1,\n",
        "      \"config_list\": [{\"lora_config_path\": adapter_path}]\n",
        "    }\n",
        "\n",
        "  ##############################################################################\n",
        "  ################################# DATASET ###################################\n",
        "  ##############################################################################\n",
        "  def transform_single_sa_to_aa(sequence):\n",
        "    \"\"\"\n",
        "    Remove the # and lower alphabet in sequence\n",
        "    \"\"\"\n",
        "    return ''.join(char for char in sequence if char.isupper() and char != \"#\")\n",
        "\n",
        "  def transform_sa_to_aa(df):\n",
        "    for column in df.columns:\n",
        "      if 'sequence' in column:\n",
        "        df[column] = df[column].apply(transform_single_sa_to_aa)\n",
        "    return df\n",
        "\n",
        "  if data_type in data_type_list_multiple:\n",
        "    csv_dataset_path = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "    df = read_csv_dataset(csv_dataset_path)\n",
        "    df = transform_sa_to_aa(df)\n",
        "  else:\n",
        "    single_sa_seq = get_SA_sequence_by_data_type(data_type, raw_data)\n",
        "    if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "      single_sa_seq[0] = transform_single_sa_to_aa(single_sa_seq[0])\n",
        "      single_sa_seq[1] = transform_single_sa_to_aa(single_sa_seq[1])\n",
        "      df = pd.DataFrame({\n",
        "          'sequence_1': [single_sa_seq[0]],\n",
        "          'sequence_2': [single_sa_seq[1]]\n",
        "      })\n",
        "      df = transform_sa_to_aa(df)\n",
        "    else:\n",
        "      single_sa_seq = transform_single_sa_to_aa(single_sa_seq)\n",
        "      df = pd.DataFrame({\n",
        "          'sequence': [single_sa_seq]\n",
        "      })\n",
        "      df = transform_sa_to_aa(df)\n",
        "\n",
        "  # def mask_struc_token(sequence):\n",
        "  #   return ''.join('#' if i % 2 == 1 and char.islower() else char for i, char in enumerate(sequence))\n",
        "\n",
        "  # if (required_training_data_type == \"AA\") and (\"AA\" not in data_type):\n",
        "  #   if 'sequence' in df.columns:\n",
        "  #     df['sequence'] = df['sequence'].apply(mask_struc_token)\n",
        "  #   elif 'sequence_1' in df.columns and 'sequence_2' in df.columns:\n",
        "  #     df['sequence_1'] = df['sequence_1'].apply(mask_struc_token)\n",
        "  #     df['sequence_2'] = df['sequence_2'].apply(mask_struc_token)\n",
        "\n",
        "  ################################################################################\n",
        "  ##################################### CONFIG ###################################\n",
        "  ################################################################################\n",
        "  from saprot.config.config_dict import Default_config\n",
        "  config = copy.deepcopy(Default_config)\n",
        "\n",
        "  # task\n",
        "  if task_type in [ \"classification\", \"token_classification\", \"pair_classification\"]:\n",
        "    config.model.kwargs.num_labels = num_of_categories.value\n",
        "  # base model\n",
        "  config.model.model_py_path = model_type_dict[task_type]\n",
        "  config.model.kwargs.config_path = base_model\n",
        "  # lora\n",
        "  config.model.kwargs.lora_kwargs = lora_kwargs\n",
        "\n",
        "  ################################################################################\n",
        "  ################################### LOAD MODEL ##################################\n",
        "  ################################################################################\n",
        "  model = my_load_model(config.model)\n",
        "  tokenizer = EsmTokenizer.from_pretrained(config.model.kwargs.config_path)\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "\n",
        "  ################################################################################\n",
        "  ################################### INFO #######################################\n",
        "  ################################################################################\n",
        "  # clear_output(wait=True)\n",
        "  print('\\n')\n",
        "  print('='*100)\n",
        "\n",
        "  print(Fore.BLUE+f\"Task Type: {original_task_type}\"+Style.RESET_ALL)\n",
        "\n",
        "  print(Fore.BLUE+f\"Model ({use_model_from}):\"+Style.RESET_ALL)\n",
        "  if multi_lora:\n",
        "    print(Fore.BLUE+f\"  Base Model: {base_model}\"+Style.RESET_ALL)\n",
        "    print(Fore.BLUE+f\"  Adapter:\"+Style.RESET_ALL)\n",
        "    for lora_config in lora_kwargs.config_list:\n",
        "      print(Fore.BLUE+f\"    {lora_config.lora_config_path}\"+Style.RESET_ALL)\n",
        "  else:\n",
        "    print(Fore.BLUE+f\"  Base Model: {base_model}\"+Style.RESET_ALL)\n",
        "    print(Fore.BLUE+f\"  Adapter: {adapter_path}\"+Style.RESET_ALL)\n",
        "\n",
        "  print(Fore.BLUE+f'Dataset ({data_type}):' +Style.RESET_ALL)\n",
        "  if mode == \"Multiple Sequences\":\n",
        "    print(Fore.BLUE+f\"  CSV Dataset Path: {csv_dataset_path}\"+Style.RESET_ALL)\n",
        "  else:\n",
        "    if \"A pair of\" in data_type:\n",
        "      print(Fore.BLUE+f\"  Sequence 1: {single_sa_seq[0]}\"+Style.RESET_ALL)\n",
        "      print(Fore.BLUE+f\"  Sequence 2: {single_sa_seq[1]}\"+Style.RESET_ALL)\n",
        "    else:\n",
        "      print(Fore.BLUE+f\"  Sequence: {single_sa_seq}\"+Style.RESET_ALL)\n",
        "\n",
        "  ################################################################################\n",
        "  ################################### INFERENCE ##################################\n",
        "  ################################################################################\n",
        "  print()\n",
        "  print('='*100)\n",
        "  print(Fore.BLUE+\"Prediction Result:\"+Style.RESET_ALL)\n",
        "\n",
        "  outputs_list=[]\n",
        "  if task_type in [\"pair_classification\", \"pair_regression\"]:\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "      input_1 = tokenizer(row['sequence_1'], return_tensors=\"pt\")\n",
        "      input_1 = {k: v.to(device) for k, v in input_1.items()}\n",
        "      input_2 = tokenizer(row['sequence_2'], return_tensors=\"pt\")\n",
        "      input_2 = {k: v.to(device) for k, v in input_2.items()}\n",
        "\n",
        "      with torch.no_grad(): outputs = model(input_1, input_2)\n",
        "      outputs_list.append(outputs)\n",
        "  else:\n",
        "    for index in tqdm(range(len(df))):\n",
        "      seq = df['sequence'].iloc[index]\n",
        "      inputs = tokenizer(seq, return_tensors=\"pt\")\n",
        "      inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "      with torch.no_grad(): outputs = model(inputs)\n",
        "      outputs_list.append(outputs)\n",
        "\n",
        "  ################################################################################\n",
        "  ################################### RESULT ##################################\n",
        "  ################################################################################\n",
        "  timestamp = str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
        "  output_file = OUTPUT_HOME / f'output_{timestamp}.csv'\n",
        "\n",
        "  if task_type == \"pair_classification\":\n",
        "    softmax_output_list = [F.softmax(output, dim=1).squeeze().tolist() for output in outputs_list]\n",
        "    print()\n",
        "    for index, output in enumerate(softmax_output_list):\n",
        "      print(f\"For Sequence Pair {index}, Category {output.index(max(output))}, Probability: {output}\")\n",
        "      df.loc[index, 'result'] = output.index(max(output))\n",
        "      df.loc[index, 'probability'] = ', '.join(map(str, output))\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "  elif task_type == \"pair_regression\":\n",
        "    print()\n",
        "    for index, output in enumerate(outputs_list):\n",
        "      print(f\"For Sequence Pair {index}, Value {output.cpu().item()}\")\n",
        "    df['score'] = [output.cpu().item() for output in outputs_list]\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "  elif task_type == \"classification\":\n",
        "    print()\n",
        "    softmax_output_list = [F.softmax(output, dim=1).squeeze().tolist() for output in outputs_list]\n",
        "    for index, output in enumerate(softmax_output_list):\n",
        "      print(f\"For Sequence {index}, Category {output.index(max(output))}, Probability: {output}\")\n",
        "      df.loc[index, 'result'] = output.index(max(output))\n",
        "      df.loc[index, 'probability'] = ', '.join(map(str, output))\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "  elif task_type == \"regression\":\n",
        "    print()\n",
        "    for index, output in enumerate(outputs_list):\n",
        "      print(f\"For Sequence {index}, Value {output.item()}\")\n",
        "    df['score'] = [output.cpu().item() for output in outputs_list]\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "  elif task_type == \"token_classification\":\n",
        "    seq_prob_df_list = []\n",
        "    softmax_output_list = [F.softmax(output, dim=-1).squeeze().tolist() for output in outputs_list]\n",
        "    # print(\"The probability of each category:\")\n",
        "    for seq_index, seq in enumerate(softmax_output_list):\n",
        "      seq_prob_df = pd.DataFrame(seq)[1:-1]\n",
        "      # print('='*100)\n",
        "      # print(f'Sequence {seq_index + 1}:')\n",
        "      # print(seq_prob_df.to_string())\n",
        "      seq_prob_df['seq_index'] = seq_index\n",
        "      seq_prob_df['aa_index'] = seq_prob_df.index\n",
        "      seq_prob_df['sequence'] = df.loc[seq_index, 'sequence']\n",
        "      seq_prob_df_list.append(seq_prob_df)\n",
        "    combined_df = pd.concat(seq_prob_df_list, ignore_index=False)\n",
        "    combined_df.to_csv(output_file, index=True)\n",
        "\n",
        "  print()\n",
        "  print('='*100)\n",
        "  print(Fore.BLUE+f\"The prediction result is saved to {output_file} and your local computer.\"+Style.RESET_ALL)\n",
        "  file_download(output_file)\n",
        "\n",
        "################################################################################\n",
        "#################################### BUTTON #################################\n",
        "################################################################################\n",
        "button_predict = ipywidgets.Button(\n",
        "    description='Make Prediction',\n",
        "    disabled=False,\n",
        "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Apply',\n",
        "    # icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "    )\n",
        "button_predict.on_click(predict)\n",
        "# button_predict.layout.width = '500px'\n",
        "display(button_predict)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "westlake",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}