import torchmetrics
import torch
import torch.distributed as dist

from torch.nn.functional import cross_entropy
from ..model_interface import register_model
from .base import SaprotBaseModel


@register_model
class SaprotClassificationModel(SaprotBaseModel):
    def __init__(self, num_labels: int, **kwargs):
        """
        Args:
            num_labels: number of labels
            **kwargs: other arguments for SaprotBaseModel
        """
        self.num_labels = num_labels
        # Cache for ESM3 feature dimensions to ensure consistency
        self._feature_dim_cache = None
        self._esm3_encoding_cache = {}
        super().__init__(task="classification", **kwargs)
        
        # é¢„å…ˆåˆ›å»ºåˆ†ç±»å¤´ï¼Œä½¿ç”¨é»˜è®¤ç»´åº¦ï¼Œç¨åä¼šåœ¨first forwardæ—¶è°ƒæ•´
        # è¿™ç¡®ä¿åˆ†ç±»å¤´ä»ä¸€å¼€å§‹å°±åœ¨æ¨¡å‹å‚æ•°ä¸­
        default_input_dim = 2560  # ESM3çš„é»˜è®¤è¾“å‡ºç»´åº¦
        self.classification_head = torch.nn.Linear(default_input_dim, self.num_labels)
        print(f"é¢„åˆ›å»ºåˆ†ç±»å¤´ï¼Œè¾“å…¥ç»´åº¦: {default_input_dim}, è¾“å‡ºç»´åº¦: {self.num_labels}")
        
    def initialize_metrics(self, stage):
        # For newer versions of torchmetrics, need to specify task type
        if self.num_labels == 2:
            task = "binary"
        else:
            task = "multiclass"
        
        return {f"{stage}_acc": torchmetrics.Accuracy(task=task, num_classes=self.num_labels)}

    def setup(self, stage=None):
        """PyTorch Lightningçš„setupæ–¹æ³•ï¼Œåœ¨è¿™é‡Œè®¾ç½®ESM3æ¨¡å‹åˆ°æ•°æ®é›†"""
        super().setup(stage)
        
        # å»¶è¿Ÿè®¾ç½®ESM3æ¨¡å‹åˆ°æ•°æ®é›†ï¼Œå› ä¸ºæ•°æ®é›†å®ä¾‹åœ¨dataloaderåˆ›å»ºæ—¶æ‰ç”Ÿæˆ
        print("æ¨¡å‹setupå®Œæˆï¼Œå°†åœ¨è®­ç»ƒå¼€å§‹æ—¶è®¾ç½®ESM3æ¨¡å‹åˆ°æ•°æ®é›†")

    def on_train_start(self):
        """è®­ç»ƒå¼€å§‹æ—¶çš„å›è°ƒï¼Œç¡®ä¿ESM3æ¨¡å‹ä¼ é€’ç»™æ•°æ®é›†"""
        super().on_train_start()
        
        # è®¾ç½®ESM3æ¨¡å‹åˆ°æ‰€æœ‰æ•°æ®é›†
        self._set_esm_model_to_datasets()

    def on_validation_start(self):
        """éªŒè¯å¼€å§‹æ—¶çš„å›è°ƒï¼Œç¡®ä¿ESM3æ¨¡å‹ä¼ é€’ç»™æ•°æ®é›†"""
        super().on_validation_start()
        
        # è®¾ç½®ESM3æ¨¡å‹åˆ°æ‰€æœ‰æ•°æ®é›†
        self._set_esm_model_to_datasets()

    def on_test_start(self):
        """æµ‹è¯•å¼€å§‹æ—¶çš„å›è°ƒï¼Œç¡®ä¿ESM3æ¨¡å‹ä¼ é€’ç»™æ•°æ®é›†"""
        super().on_test_start()
        
        # è®¾ç½®ESM3æ¨¡å‹åˆ°æ‰€æœ‰æ•°æ®é›†
        self._set_esm_model_to_datasets()

    def _set_esm_model_to_datasets(self):
        """å°†ESM3æ¨¡å‹è®¾ç½®åˆ°æ‰€æœ‰æ•°æ®é›†"""
        if hasattr(self.trainer, 'datamodule'):
            datasets = []
            
            # è·å–æ‰€æœ‰æ•°æ®é›†å®ä¾‹
            if hasattr(self.trainer.datamodule, 'train_dataset'):
                datasets.append(('train', self.trainer.datamodule.train_dataset))
            if hasattr(self.trainer.datamodule, 'val_dataset'):
                datasets.append(('val', self.trainer.datamodule.val_dataset))
            if hasattr(self.trainer.datamodule, 'test_dataset'):
                datasets.append(('test', self.trainer.datamodule.test_dataset))
            
            # è®¾ç½®ESM3æ¨¡å‹
            for stage, dataset in datasets:
                if dataset is not None and hasattr(dataset, 'set_esm_model'):
                    print(f"è®¾ç½®ESM3æ¨¡å‹åˆ°{stage}æ•°æ®é›†: {type(dataset).__name__}")
                    dataset.set_esm_model(self.model)
                    
            # å¦å¤–æ£€æŸ¥dataloaderä¸­çš„æ•°æ®é›†
            dataloaders = []
            
            # å®‰å…¨åœ°è·å–dataloaders
            if hasattr(self.trainer, 'train_dataloader') and self.trainer.train_dataloader is not None:
                train_dl = self.trainer.train_dataloader
                if callable(train_dl):
                    train_dl = train_dl()
                dataloaders.append(('train', train_dl))
                
            if hasattr(self.trainer, 'val_dataloaders') and self.trainer.val_dataloaders is not None:
                val_dl = self.trainer.val_dataloaders
                if callable(val_dl):
                    val_dl = val_dl()
                # val_dataloaderså¯èƒ½æ˜¯åˆ—è¡¨
                if isinstance(val_dl, list):
                    for i, dl in enumerate(val_dl):
                        dataloaders.append((f'val_{i}', dl))
                else:
                    dataloaders.append(('val', val_dl))
                    
            if hasattr(self.trainer, 'test_dataloaders') and self.trainer.test_dataloaders is not None:
                test_dl = self.trainer.test_dataloaders
                if callable(test_dl):
                    test_dl = test_dl()
                # test_dataloaderså¯èƒ½æ˜¯åˆ—è¡¨
                if isinstance(test_dl, list):
                    for i, dl in enumerate(test_dl):
                        dataloaders.append((f'test_{i}', dl))
                else:
                    dataloaders.append(('test', test_dl))
            
            for stage, dataloader in dataloaders:
                if dataloader is not None:
                    if hasattr(dataloader, 'dataset') and hasattr(dataloader.dataset, 'set_esm_model'):
                        print(f"è®¾ç½®ESM3æ¨¡å‹åˆ°{stage} dataloaderæ•°æ®é›†: {type(dataloader.dataset).__name__}")
                        dataloader.dataset.set_esm_model(self.model)

    def forward(self, inputs=None, coords=None, sequences=None, embeddings=None, encoded_proteins=None, **kwargs):
        # Handle different input formats
        if inputs is None and sequences is not None:
            inputs = {"sequences": sequences}
        elif inputs is None and embeddings is not None:
            inputs = {"embeddings": embeddings}
        elif inputs is None and encoded_proteins is not None:
            inputs = {"encoded_proteins": encoded_proteins}
        elif inputs is None:
            inputs = kwargs
        
        if coords is not None:
            inputs = self.add_bias_feature(inputs, coords)
        
        # Get device and dtype from model parameters
        device = next(self.model.parameters()).device
        model_dtype = next(self.model.parameters()).dtype
        
        # ä¼˜å…ˆå¤„ç†encoded_proteins
        if "encoded_proteins" in inputs:
            print(f"[æ¨¡å‹è°ƒè¯•] ä½¿ç”¨encoded_proteinsï¼Œæ•°é‡: {len(inputs['encoded_proteins'])}")
            encoded_proteins = inputs["encoded_proteins"]
            
            # ä½¿ç”¨ESM3æ¨¡å‹çš„forwardæ–¹æ³•å¤„ç†encoded_proteins
            features = []
            for i, encoded_protein in enumerate(encoded_proteins):
                try:
                    with torch.no_grad():
                        # ä½¿ç”¨æ¨¡å‹çš„forwardæ–¹æ³•å¤„ç†encoded_protein
                        output = self.model.forward(encoded_protein)
                        
                        # ä»è¾“å‡ºä¸­æå–åµŒå…¥
                        if hasattr(output, 'embeddings'):
                            embedding = output.embeddings
                        elif hasattr(output, 'last_hidden_state'):
                            embedding = output.last_hidden_state
                        elif hasattr(output, 'sequence_embeddings'):
                            embedding = output.sequence_embeddings
                        else:
                            print(f"[æ¨¡å‹è°ƒè¯•] encoded_protein {i} æ— æ³•æ‰¾åˆ°åµŒå…¥ï¼Œä½¿ç”¨é›¶å‘é‡")
                            embedding = torch.zeros(2560, device=device, dtype=model_dtype)
                        
                        # ç¡®ä¿æ­£ç¡®çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹
                        if torch.is_tensor(embedding):
                            embedding = embedding.to(device=device, dtype=model_dtype)
                            # åº”ç”¨å¹³å‡æ± åŒ–
                            if embedding.dim() == 3:  # [batch, seq_len, hidden_dim]
                                if embedding.shape[0] == 1:
                                    embedding = embedding.squeeze(0)  # [seq_len, hidden_dim]
                                feature = embedding.mean(dim=0)  # [hidden_dim]
                            elif embedding.dim() == 2:  # [seq_len, hidden_dim]
                                feature = embedding.mean(dim=0)  # [hidden_dim]
                            elif embedding.dim() == 1:  # [hidden_dim]
                                feature = embedding
                            else:
                                feature = embedding.flatten()
                        else:
                            feature = torch.zeros(2560, device=device, dtype=model_dtype)
                        
                        features.append(feature)
                        print(f"[æ¨¡å‹è°ƒè¯•] encoded_protein {i} å¤„ç†å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {feature.shape}")
                        
                except Exception as e:
                    print(f"[æ¨¡å‹è°ƒè¯•] encoded_protein {i} å¤„ç†å¤±è´¥: {str(e)}")
                    feature = torch.zeros(2560, device=device, dtype=model_dtype)
                    features.append(feature)
            
            if features:
                stacked_features = torch.stack(features)
            else:
                stacked_features = torch.zeros(1, 2560, device=device, dtype=model_dtype)
        
        # ä¼˜å…ˆå¤„ç†é¢„ç¼–ç çš„åµŒå…¥
        elif "embeddings" in inputs:
            print(f"[æ¨¡å‹è°ƒè¯•] ä½¿ç”¨é¢„ç¼–ç çš„åµŒå…¥ï¼Œå½¢çŠ¶: {inputs['embeddings'].shape}")
            stacked_features = inputs["embeddings"].to(device=device, dtype=model_dtype)
        
        elif "sequences" in inputs:
            print(f"[æ¨¡å‹è°ƒè¯•] å¤„ç†åŸå§‹åºåˆ—ï¼Œæ•°é‡: {len(inputs['sequences'])}")
            sequences = inputs["sequences"]
            
            # Process sequences using ESM3 in the model
            from esm.sdk.api import ESMProtein
            
            features = []
            for i, seq in enumerate(sequences):
                try:
                    protein = ESMProtein(sequence=seq)
                    with torch.no_grad():
                        encoded_protein = self.model.encode(protein)
                    
                    # Extract sequence embeddings
                    if hasattr(encoded_protein, 'sequence'):
                        seq_repr = encoded_protein.sequence
                        if torch.is_tensor(seq_repr):
                            # Apply mean pooling if it's a sequence of embeddings
                            if seq_repr.dim() > 1:
                                feature = seq_repr.mean(dim=0)
                            else:
                                feature = seq_repr
                        else:
                            # Convert to tensor with proper device and dtype
                            tensor_repr = torch.tensor(seq_repr, device=device, dtype=model_dtype)
                            if tensor_repr.dim() > 1:
                                feature = tensor_repr.mean(dim=0)
                            else:
                                feature = tensor_repr
                        
                        features.append(feature.to(device=device, dtype=model_dtype))
                        print(f"[æ¨¡å‹è°ƒè¯•] åºåˆ— {i} ç¼–ç å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {feature.shape}")
                    else:
                        print(f"[æ¨¡å‹è°ƒè¯•] åºåˆ— {i} ç¼–ç å¤±è´¥ï¼Œä½¿ç”¨é›¶å‘é‡")
                        feature = torch.zeros(2560, device=device, dtype=model_dtype)
                        features.append(feature)
                except Exception as e:
                    print(f"[æ¨¡å‹è°ƒè¯•] åºåˆ— {i} ç¼–ç å‡ºé”™: {str(e)}")
                    feature = torch.zeros(2560, device=device, dtype=model_dtype)
                    features.append(feature)
            
            if features:
                stacked_features = torch.stack(features)
            else:
                stacked_features = torch.zeros(1, 2560, device=device, dtype=model_dtype)
        
        else:
            print(f"[æ¨¡å‹è°ƒè¯•] âŒ è¾“å…¥ä¸­æ²¡æœ‰æ‰¾åˆ°encoded_proteinsã€embeddingsæˆ–sequences")
            stacked_features = torch.zeros(1, 2560, device=device, dtype=model_dtype)
        
        # Ensure stacked_features is on the correct device and dtype
        stacked_features = stacked_features.to(device=device, dtype=model_dtype)
        
        # Get the actual input dimension from the features
        actual_input_dim = stacked_features.shape[-1]
        print(f"[æ¨¡å‹è°ƒè¯•] ç‰¹å¾ç»´åº¦: {stacked_features.shape}, åˆ†ç±»å¤´è¾“å…¥ç»´åº¦: {self.classification_head.in_features}")
        
        # æ£€æŸ¥åˆ†ç±»å¤´çš„è¾“å…¥ç»´åº¦æ˜¯å¦åŒ¹é…ï¼Œå¦‚æœä¸åŒ¹é…åˆ™é‡å»º
        if self.classification_head.in_features != actual_input_dim:
            print(f"[æ¨¡å‹è°ƒè¯•] ğŸ”§ é‡å»ºåˆ†ç±»å¤´: {self.classification_head.in_features} -> {actual_input_dim}")
            self.classification_head = torch.nn.Linear(actual_input_dim, self.num_labels)
            self.classification_head = self.classification_head.to(device=device, dtype=model_dtype)
            
            # æ›´æ–°feature cache
            self._feature_dim_cache = actual_input_dim
            print(f"[æ¨¡å‹è°ƒè¯•] âœ… åˆ†ç±»å¤´é‡å»ºå®Œæˆ")
            
            # é‡æ–°é…ç½®ä¼˜åŒ–å™¨ä»¥åŒ…å«æ–°çš„åˆ†ç±»å¤´å‚æ•°
            self._reconfigure_optimizer()
        
        # ç¡®ä¿åˆ†ç±»å¤´åœ¨æ­£ç¡®çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹ä¸Š
        self.classification_head = self.classification_head.to(device=device, dtype=model_dtype)
        
        # Forward pass
        logits = self.classification_head(stacked_features)
        print(f"[æ¨¡å‹è°ƒè¯•] åˆ†ç±»è¾“å‡ºå½¢çŠ¶: {logits.shape}")
        
        return logits

    def _reconfigure_optimizer(self):
        """é‡æ–°é…ç½®ä¼˜åŒ–å™¨ä»¥åŒ…å«åˆ†ç±»å¤´å‚æ•°"""
        if hasattr(self, 'trainer') and self.trainer is not None and hasattr(self, 'optimizers'):
            print("é‡æ–°é…ç½®ä¼˜åŒ–å™¨ä»¥åŒ…å«åˆ†ç±»å¤´å‚æ•°")
            
            # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨ä»¥åŒ…å«æ–°çš„å‚æ•°
            self.init_optimizers()
            
            # å¦‚æœè®­ç»ƒå™¨å­˜åœ¨ï¼Œæ›´æ–°è®­ç»ƒå™¨çš„ä¼˜åŒ–å™¨é…ç½®
            if hasattr(self.trainer, 'strategy'):
                self.trainer.strategy.optimizers = [self.optimizer]
                print("ä¼˜åŒ–å™¨é‡æ–°é…ç½®å®Œæˆ")

    def loss_func(self, stage, logits, labels):
        label = labels['labels']
        
        # Compute loss
        loss = cross_entropy(logits, label)
        
        # Update metrics - convert logits to predictions for metric calculation
        # Convert to float32 for metric computation to avoid precision issues
        with torch.no_grad():
            preds = torch.argmax(logits.float(), dim=-1)
            for metric in self.metrics[stage].values():
                metric.update(preds, label)

        if stage == "train":
            log_dict = self.get_log_dict("train")
            log_dict["train_loss"] = loss
            self.log_info(log_dict)

            # Reset train metrics
            self.reset_metrics("train")

        return loss

    def on_train_epoch_end(self):
        """è®­ç»ƒepochç»“æŸæ—¶çš„å›è°ƒ"""
        super().on_train_epoch_end()  # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        # æ‰“å°åˆ†ç±»å¤´æƒé‡ä¿¡æ¯
        self._print_classification_head_weights("è®­ç»ƒ")

    def on_test_epoch_end(self):
        # æ‰“å°åˆ†ç±»å¤´æƒé‡ä¿¡æ¯
        self._print_classification_head_weights("æµ‹è¯•")
        
        log_dict = self.get_log_dict("test")
        # log_dict["test_loss"] = torch.cat(self.all_gather(self.test_outputs), dim=-1).mean()
        log_dict["test_loss"] = torch.mean(torch.stack(self.test_outputs))

        # if dist.get_rank() == 0:
        #     print(log_dict)
        self.output_test_metrics(log_dict)
        self.log_info(log_dict)
        self.reset_metrics("test")

    def on_validation_epoch_end(self):
        # æ‰“å°åˆ†ç±»å¤´æƒé‡ä¿¡æ¯
        self._print_classification_head_weights("éªŒè¯")
        
        log_dict = self.get_log_dict("valid")
        # log_dict["valid_loss"] = torch.cat(self.all_gather(self.valid_outputs), dim=-1).mean()
        log_dict["valid_loss"] = torch.mean(torch.stack(self.valid_outputs))

        # if dist.get_rank() == 0:
        #     print(log_dict)
        self.log_info(log_dict)
        self.reset_metrics("valid")
        self.check_save_condition(log_dict["valid_acc"], mode="max")

        self.plot_valid_metrics_curve(log_dict)

    def _print_classification_head_weights(self, stage_name):
        """æ‰“å°åˆ†ç±»å¤´æƒé‡ç»Ÿè®¡ä¿¡æ¯"""
        if hasattr(self, 'classification_head') and self.classification_head is not None:
            weight = self.classification_head.weight
            bias = self.classification_head.bias
            
            print(f"\n=== {stage_name}é˜¶æ®µç»“æŸ - åˆ†ç±»å¤´æƒé‡ç»Ÿè®¡ (Epoch {self.current_epoch}) ===")
            print(f"æƒé‡çŸ©é˜µå½¢çŠ¶: {weight.shape}")
            print(f"æƒé‡ç»Ÿè®¡: min={weight.min().item():.6f}, max={weight.max().item():.6f}, mean={weight.mean().item():.6f}, std={weight.std().item():.6f}")
            print(f"æƒé‡æ¢¯åº¦ç»Ÿè®¡: {'æœ‰æ¢¯åº¦' if weight.grad is not None else 'æ— æ¢¯åº¦'}")
            if weight.grad is not None:
                print(f"æ¢¯åº¦ç»Ÿè®¡: min={weight.grad.min().item():.6f}, max={weight.grad.max().item():.6f}, mean={weight.grad.mean().item():.6f}")
            
            if bias is not None:
                print(f"åç½®å½¢çŠ¶: {bias.shape}")
                print(f"åç½®ç»Ÿè®¡: min={bias.min().item():.6f}, max={bias.max().item():.6f}, mean={bias.mean().item():.6f}")
                print(f"åç½®æ¢¯åº¦ç»Ÿè®¡: {'æœ‰æ¢¯åº¦' if bias.grad is not None else 'æ— æ¢¯åº¦'}")
                if bias.grad is not None:
                    print(f"åç½®æ¢¯åº¦ç»Ÿè®¡: min={bias.grad.min().item():.6f}, max={bias.grad.max().item():.6f}, mean={bias.grad.mean().item():.6f}")
            
            # æ£€æŸ¥æƒé‡æ˜¯å¦åœ¨è®­ç»ƒä¸­å‘ç”Ÿå˜åŒ–
            if not hasattr(self, '_prev_weights'):
                self._prev_weights = weight.clone().detach()
                print("é¦–æ¬¡è®°å½•æƒé‡")
            else:
                weight_diff = torch.abs(weight - self._prev_weights).mean().item()
                print(f"æƒé‡å˜åŒ–é‡: {weight_diff:.8f}")
                if weight_diff < 1e-8:
                    print("âš ï¸  è­¦å‘Š: æƒé‡å‡ ä¹æ²¡æœ‰å˜åŒ–ï¼Œå¯èƒ½æ²¡æœ‰åœ¨è®­ç»ƒ!")
                else:
                    print("âœ“ æƒé‡æ­£åœ¨æ›´æ–°")
                self._prev_weights = weight.clone().detach()
            
            print("=" * 60 + "\n")
        else:
            print(f"\nâš ï¸  {stage_name}é˜¶æ®µç»“æŸ - åˆ†ç±»å¤´å°šæœªåˆ›å»º (Epoch {self.current_epoch})\n")