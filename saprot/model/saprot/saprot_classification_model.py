import torchmetrics
import torch
import torch.distributed as dist

from torch.nn.functional import cross_entropy
from ..model_interface import register_model
from .base import SaprotBaseModel
# å¯¼å…¥å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä¿®å¤å¯¼å…¥è·¯å¾„
from utils.lr_scheduler import ConstantLRScheduler, CosineAnnealingLRScheduler, Esm2LRScheduler


@register_model
class SaprotClassificationModel(SaprotBaseModel):
    def __init__(self, num_labels: int, fixed_seq_length: int = 2048, **kwargs):
        """
        Args:
            num_labels: number of labels
            fixed_seq_length: å›ºå®šåºåˆ—é•¿åº¦ï¼Œç”¨äºæˆªæ–­æˆ–padding
            **kwargs: other arguments for SaprotBaseModel
        """
        self.num_labels = num_labels
        self.fixed_seq_length = fixed_seq_length
        super().__init__(task="classification", **kwargs)
        
        # åˆ›å»ºå›ºå®šç»´åº¦çš„åˆ†ç±»å¤´
        self.classification_head = torch.nn.Linear(self.fixed_seq_length, self.num_labels)
        
        # ç«‹å³éªŒè¯åˆ†ç±»å¤´æ˜¯å¦è¢«æ­£ç¡®åˆ›å»º
        # print(f"ğŸ” ç«‹å³éªŒè¯åˆ†ç±»å¤´åˆ›å»º...")
        # print(f"åˆ†ç±»å¤´å­˜åœ¨: {hasattr(self, 'classification_head')}")
        # print(f"åˆ†ç±»å¤´ä¸ä¸ºNone: {self.classification_head is not None}")
        
        if self.classification_head is not None:
            param_list = list(self.classification_head.parameters())
            # print(f"åˆ†ç±»å¤´å‚æ•°æ•°é‡: {len(param_list)}")
            # for i, param in enumerate(param_list):
            #     print(f"  å‚æ•° {i}: shape={param.shape}, requires_grad={param.requires_grad}, device={param.device}")
        
        # ç¡®ä¿åˆ†ç±»å¤´å‚æ•°å¯ä»¥è®­ç»ƒ
        # for name, param in self.classification_head.named_parameters():
        #     print(f"è®¾ç½®å‚æ•° {name} çš„ requires_grad=True")
        #     param.requires_grad = True
        #     print(f"éªŒè¯å‚æ•° {name}: requires_grad={param.requires_grad}")
            
        # print(f"åˆ›å»ºå›ºå®šåˆ†ç±»å¤´: {self.fixed_seq_length} -> {self.num_labels}")
        # print(f"åˆ†ç±»å¤´å‚æ•°: weight={self.classification_head.weight.shape}, bias={self.classification_head.bias.shape}")
        # print(f"åˆ†ç±»å¤´å‚æ•°requires_grad: weight={self.classification_head.weight.requires_grad}, bias={self.classification_head.bias.requires_grad}")
        
        # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨ä»¥åŒ…å«åˆ†ç±»å¤´å‚æ•°
        # print("é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨ä»¥åŒ…å«åˆ†ç±»å¤´å‚æ•°...")
        self.init_optimizers()
        # print("ä¼˜åŒ–å™¨é‡æ–°åˆå§‹åŒ–å®Œæˆ")
        
    def initialize_metrics(self, stage):
        # For newer versions of torchmetrics, need to specify task type
        if self.num_labels == 2:
            task = "binary"
        else:
            task = "multiclass"
        
        return {f"{stage}_acc": torchmetrics.Accuracy(task=task, num_classes=self.num_labels)}

    def setup(self, stage=None):
        """PyTorch Lightningçš„setupæ–¹æ³•ï¼Œåœ¨è¿™é‡Œè®¾ç½®ESM3æ¨¡å‹åˆ°æ•°æ®é›†"""
        super().setup(stage)
        # print("æ¨¡å‹setupå®Œæˆï¼Œå°†åœ¨è®­ç»ƒå¼€å§‹æ—¶è®¾ç½®ESM3æ¨¡å‹åˆ°æ•°æ®é›†")

    def on_train_start(self):
        """è®­ç»ƒå¼€å§‹æ—¶çš„å›è°ƒï¼Œç¡®ä¿ESM3æ¨¡å‹ä¼ é€’ç»™æ•°æ®é›†"""
        super().on_train_start()
        self._set_esm_model_to_datasets()
        
        # éªŒè¯åˆ†ç±»å¤´å‚æ•°æ˜¯å¦åœ¨ä¼˜åŒ–å™¨ä¸­
        # self._verify_classification_head_in_optimizer()

    def on_validation_start(self):
        """éªŒè¯å¼€å§‹æ—¶çš„å›è°ƒï¼Œç¡®ä¿ESM3æ¨¡å‹ä¼ é€’ç»™æ•°æ®é›†"""
        super().on_validation_start()
        self._set_esm_model_to_datasets()

    def on_test_start(self):
        """æµ‹è¯•å¼€å§‹æ—¶çš„å›è°ƒï¼Œç¡®ä¿ESM3æ¨¡å‹ä¼ é€’ç»™æ•°æ®é›†"""
        super().on_test_start()
        self._set_esm_model_to_datasets()

    def _set_esm_model_to_datasets(self):
        """å°†ESM3æ¨¡å‹è®¾ç½®åˆ°æ‰€æœ‰æ•°æ®é›†"""
        if hasattr(self.trainer, 'datamodule'):
            datasets = []
            
            # è·å–æ‰€æœ‰æ•°æ®é›†å®ä¾‹
            if hasattr(self.trainer.datamodule, 'train_dataset'):
                datasets.append(('train', self.trainer.datamodule.train_dataset))
            if hasattr(self.trainer.datamodule, 'val_dataset'):
                datasets.append(('val', self.trainer.datamodule.val_dataset))
            if hasattr(self.trainer.datamodule, 'test_dataset'):
                datasets.append(('test', self.trainer.datamodule.test_dataset))
            
            # è®¾ç½®ESM3æ¨¡å‹
            for stage, dataset in datasets:
                if dataset is not None and hasattr(dataset, 'set_esm_model'):
                    # print(f"è®¾ç½®ESM3æ¨¡å‹åˆ°{stage}æ•°æ®é›†: {type(dataset).__name__}")
                    dataset.set_esm_model(self.model)
                    
            # å¦å¤–æ£€æŸ¥dataloaderä¸­çš„æ•°æ®é›†
            dataloaders = []
            
            # å®‰å…¨åœ°è·å–dataloaders
            if hasattr(self.trainer, 'train_dataloader') and self.trainer.train_dataloader is not None:
                train_dl = self.trainer.train_dataloader
                if callable(train_dl):
                    train_dl = train_dl()
                dataloaders.append(('train', train_dl))
                
            if hasattr(self.trainer, 'val_dataloaders') and self.trainer.val_dataloaders is not None:
                val_dl = self.trainer.val_dataloaders
                if callable(val_dl):
                    val_dl = val_dl()
                # val_dataloaderså¯èƒ½æ˜¯åˆ—è¡¨
                if isinstance(val_dl, list):
                    for i, dl in enumerate(val_dl):
                        dataloaders.append((f'val_{i}', dl))
                else:
                    dataloaders.append(('val', val_dl))
                    
            if hasattr(self.trainer, 'test_dataloaders') and self.trainer.test_dataloaders is not None:
                test_dl = self.trainer.test_dataloaders
                if callable(test_dl):
                    test_dl = test_dl()
                # test_dataloaderså¯èƒ½æ˜¯åˆ—è¡¨
                if isinstance(test_dl, list):
                    for i, dl in enumerate(test_dl):
                        dataloaders.append((f'test_{i}', dl))
                else:
                    dataloaders.append(('test', test_dl))
            
            for stage, dataloader in dataloaders:
                if dataloader is not None:
                    if hasattr(dataloader, 'dataset') and hasattr(dataloader.dataset, 'set_esm_model'):
                        # print(f"è®¾ç½®ESM3æ¨¡å‹åˆ°{stage} dataloaderæ•°æ®é›†: {type(dataloader.dataset).__name__}")
                        dataloader.dataset.set_esm_model(self.model)

    def _pad_or_truncate_features(self, features, target_length):
        """
        å°†ç‰¹å¾æˆªæ–­æˆ–paddingåˆ°å›ºå®šé•¿åº¦
        Args:
            features: è¾“å…¥ç‰¹å¾ tensor [batch_size, seq_len] æˆ– [batch_size, seq_len, hidden_dim]
            target_length: ç›®æ ‡é•¿åº¦
        Returns:
            å¤„ç†åçš„ç‰¹å¾ [batch_size, target_length] æˆ– [batch_size, target_length, hidden_dim]
        """
        if features.dim() == 2:
            # [batch_size, seq_len] çš„æƒ…å†µ
            batch_size, seq_len = features.shape
            if seq_len > target_length:
                # æˆªæ–­
                return features[:, :target_length]
            elif seq_len < target_length:
                # padding
                padding_size = target_length - seq_len
                padding = torch.zeros(batch_size, padding_size, device=features.device, dtype=features.dtype)
                return torch.cat([features, padding], dim=1)
            else:
                return features
        elif features.dim() == 3:
            # [batch_size, seq_len, hidden_dim] çš„æƒ…å†µï¼Œå…ˆå¹³å‡æ± åŒ–
            features = features.mean(dim=2)  # [batch_size, seq_len]
            return self._pad_or_truncate_features(features, target_length)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç‰¹å¾ç»´åº¦: {features.shape}")

    def forward(self, inputs=None, coords=None, sequences=None, embeddings=None, tokens=None, **kwargs):
        # Handle different input formats
        if inputs is None and sequences is not None:
            inputs = {"sequences": sequences}
        elif inputs is None and embeddings is not None:
            inputs = {"embeddings": embeddings}
        elif inputs is None and tokens is not None:
            inputs = {"tokens": tokens}
        elif inputs is None:
            inputs = kwargs
        
        if coords is not None:
            inputs = self.add_bias_feature(inputs, coords)
        
        # Get device and dtype from model parameters
        device = next(self.model.parameters()).device
        model_dtype = next(self.model.parameters()).dtype
        
        # ä¼˜å…ˆå¤„ç†tokens
        if "tokens" in inputs:
            # print(f"[æ¨¡å‹è°ƒè¯•] ä½¿ç”¨tokensï¼Œå½¢çŠ¶: {inputs['tokens'].shape}")
            tokens = inputs["tokens"].to(device=device)
            
            # å°†tokensè½¬æ¢ä¸ºæµ®ç‚¹æ•°ç±»å‹å¹¶è¿›è¡Œæˆªæ–­/padding
            try:
                tokens_float = tokens.float().to(dtype=model_dtype)
                
                if tokens_float.dim() == 2:
                    batch_size, seq_len = tokens_float.shape
                    # print(f"[æ¨¡å‹è°ƒè¯•] åŸå§‹åºåˆ—é•¿åº¦: {seq_len}, ç›®æ ‡é•¿åº¦: {self.fixed_seq_length}")
                    
                    # æˆªæ–­æˆ–paddingåˆ°å›ºå®šé•¿åº¦
                    stacked_features = self._pad_or_truncate_features(tokens_float, self.fixed_seq_length)
                    # print(f"[æ¨¡å‹è°ƒè¯•] å¤„ç†åç‰¹å¾å½¢çŠ¶: {stacked_features.shape}")
                    
                else:
                    # print(f"[æ¨¡å‹è°ƒè¯•] âŒ tokensç»´åº¦ä¸ç¬¦åˆé¢„æœŸ: {tokens_float.shape}")
                    # åˆ›å»ºå›ºå®šé•¿åº¦çš„é›¶ç‰¹å¾
                    batch_size = tokens.shape[0] if tokens.dim() > 0 else 1
                    stacked_features = torch.zeros(batch_size, self.fixed_seq_length, device=device, dtype=model_dtype)
                
            except Exception as e:
                # print(f"[æ¨¡å‹è°ƒè¯•] tokenså¤„ç†å¤±è´¥: {str(e)}")
                batch_size = tokens.shape[0] if tokens.dim() > 0 else 1
                stacked_features = torch.zeros(batch_size, self.fixed_seq_length, device=device, dtype=model_dtype)
        
        # å¤„ç†é¢„ç¼–ç çš„åµŒå…¥
        elif "embeddings" in inputs:
            # print(f"[æ¨¡å‹è°ƒè¯•] ä½¿ç”¨é¢„ç¼–ç çš„åµŒå…¥ï¼Œå½¢çŠ¶: {inputs['embeddings'].shape}")
            embeddings = inputs["embeddings"].to(device=device, dtype=model_dtype)
            # å¦‚æœæ˜¯é«˜ç»´åµŒå…¥ï¼Œéœ€è¦è½¬æ¢ä¸ºå›ºå®šé•¿åº¦
            if embeddings.dim() == 3:
                # [batch_size, seq_len, hidden_dim] -> [batch_size, seq_len]
                embeddings = embeddings.mean(dim=2)
            stacked_features = self._pad_or_truncate_features(embeddings, self.fixed_seq_length)
        
        elif "sequences" in inputs:
            # print(f"[æ¨¡å‹è°ƒè¯•] å¤„ç†åŸå§‹åºåˆ—ï¼Œæ•°é‡: {len(inputs['sequences'])}")
            sequences = inputs["sequences"]
            
            # Process sequences using ESM3 in the model
            from esm.sdk.api import ESMProtein
            
            features = []
            for i, seq in enumerate(sequences):
                try:
                    protein = ESMProtein(sequence=seq)
                    with torch.no_grad():
                        encoded_protein = self.model.encode(protein)
                    
                    # Extract sequence tokens
                    if hasattr(encoded_protein, 'sequence'):
                        seq_tokens = getattr(encoded_protein, 'sequence')
                        if torch.is_tensor(seq_tokens):
                            # ç›´æ¥ä½¿ç”¨tokensä½œä¸ºç‰¹å¾
                            seq_feature = seq_tokens.float()
                            # æˆªæ–­æˆ–paddingåˆ°å›ºå®šé•¿åº¦
                            if len(seq_feature) > self.fixed_seq_length:
                                seq_feature = seq_feature[:self.fixed_seq_length]
                            elif len(seq_feature) < self.fixed_seq_length:
                                padding_size = self.fixed_seq_length - len(seq_feature)
                                padding = torch.zeros(padding_size, device=device, dtype=model_dtype)
                                seq_feature = torch.cat([seq_feature, padding])
                            
                            features.append(seq_feature.to(device=device, dtype=model_dtype))
                            # print(f"[æ¨¡å‹è°ƒè¯•] åºåˆ— {i} ç¼–ç å®Œæˆï¼Œå›ºå®šé•¿åº¦: {seq_feature.shape}")
                        else:
                            # print(f"[æ¨¡å‹è°ƒè¯•] åºåˆ— {i} ç¼–ç å¤±è´¥ï¼Œä½¿ç”¨é›¶å‘é‡")
                            feature = torch.zeros(self.fixed_seq_length, device=device, dtype=model_dtype)
                            features.append(feature)
                    else:
                        # print(f"[æ¨¡å‹è°ƒè¯•] åºåˆ— {i} ç¼–ç å¤±è´¥ï¼Œä½¿ç”¨é›¶å‘é‡")
                        feature = torch.zeros(self.fixed_seq_length, device=device, dtype=model_dtype)
                        features.append(feature)
                except Exception as e:
                    # print(f"[æ¨¡å‹è°ƒè¯•] åºåˆ— {i} ç¼–ç å‡ºé”™: {str(e)}")
                    feature = torch.zeros(self.fixed_seq_length, device=device, dtype=model_dtype)
                    features.append(feature)
            
            if features:
                stacked_features = torch.stack(features)
            else:
                stacked_features = torch.zeros(1, self.fixed_seq_length, device=device, dtype=model_dtype)
        
        else:
            # print(f"[æ¨¡å‹è°ƒè¯•] âŒ è¾“å…¥ä¸­æ²¡æœ‰æ‰¾åˆ°tokensã€embeddingsæˆ–sequences")
            stacked_features = torch.zeros(1, self.fixed_seq_length, device=device, dtype=model_dtype)
        
        # Ensure stacked_features is on the correct device and dtype
        stacked_features = stacked_features.to(device=device, dtype=model_dtype)
        
        # print(f"[æ¨¡å‹è°ƒè¯•] æœ€ç»ˆç‰¹å¾ç»´åº¦: {stacked_features.shape} (å›ºå®šé•¿åº¦: {self.fixed_seq_length})")

        # ç¡®ä¿åˆ†ç±»å¤´åœ¨æ­£ç¡®çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹ä¸Š
        self.classification_head = self.classification_head.to(device=device, dtype=model_dtype)
        
        # Forward pass
        logits = self.classification_head(stacked_features)
        # print(f"[æ¨¡å‹è°ƒè¯•] åˆ†ç±»è¾“å‡ºå½¢çŠ¶: {logits.shape}")
        
        return logits

    def loss_func(self, stage, logits, labels):
        label = labels['labels']
        
        # Compute loss
        loss = cross_entropy(logits, label)
        
        # Update metrics - convert logits to predictions for metric calculation
        # Convert to float32 for metric computation to avoid precision issues
        with torch.no_grad():
            preds = torch.argmax(logits.float(), dim=-1)
            for metric in self.metrics[stage].values():
                metric.update(preds, label)

        if stage == "train":
            log_dict = self.get_log_dict("train")
            log_dict["train_loss"] = loss
            self.log_info(log_dict)

            # Reset train metrics
            self.reset_metrics("train")

        return loss

    def on_train_epoch_end(self):
        """è®­ç»ƒepochç»“æŸæ—¶çš„å›è°ƒ"""
        super().on_train_epoch_end()  # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        # æ‰“å°åˆ†ç±»å¤´æƒé‡ä¿¡æ¯
        # self._print_classification_head_weights("è®­ç»ƒ")

    def on_test_epoch_end(self):
        # æ‰“å°åˆ†ç±»å¤´æƒé‡ä¿¡æ¯
        # self._print_classification_head_weights("æµ‹è¯•")
        
        log_dict = self.get_log_dict("test")
        log_dict["test_loss"] = torch.mean(torch.stack(self.test_outputs))

        self.output_test_metrics(log_dict)
        self.log_info(log_dict)
        self.reset_metrics("test")

    def on_validation_epoch_end(self):
        # æ‰“å°åˆ†ç±»å¤´æƒé‡ä¿¡æ¯
        # self._print_classification_head_weights("éªŒè¯")
        
        log_dict = self.get_log_dict("valid")
        log_dict["valid_loss"] = torch.mean(torch.stack(self.valid_outputs))

        self.log_info(log_dict)
        self.reset_metrics("valid")
        self.check_save_condition(log_dict["valid_acc"], mode="max")

        self.plot_valid_metrics_curve(log_dict)

    def _print_classification_head_weights(self, stage_name):
        """æ‰“å°åˆ†ç±»å¤´æƒé‡ç»Ÿè®¡ä¿¡æ¯"""
        # if hasattr(self, 'classification_head') and self.classification_head is not None:
        #     weight = self.classification_head.weight
        #     bias = self.classification_head.bias
            
        #     print(f"\n=== {stage_name}é˜¶æ®µç»“æŸ - å›ºå®šåˆ†ç±»å¤´æƒé‡ç»Ÿè®¡ (Epoch {self.current_epoch}) ===")
        #     print(f"æƒé‡çŸ©é˜µå½¢çŠ¶: {weight.shape}")
        #     print(f"æƒé‡ç»Ÿè®¡: min={weight.min().item():.6f}, max={weight.max().item():.6f}, mean={weight.mean().item():.6f}, std={weight.std().item():.6f}")
        #     print(f"æƒé‡æ¢¯åº¦ç»Ÿè®¡: {'æœ‰æ¢¯åº¦' if weight.grad is not None else 'æ— æ¢¯åº¦'}")
        #     if weight.grad is not None:
        #         print(f"æ¢¯åº¦ç»Ÿè®¡: min={weight.grad.min().item():.6f}, max={weight.grad.max().item():.6f}, mean={weight.grad.mean().item():.6f}")
        #         print(f"æ¢¯åº¦èŒƒæ•°: {weight.grad.norm().item():.6f}")
            
        #     if bias is not None:
        #         print(f"åç½®å½¢çŠ¶: {bias.shape}")
        #         print(f"åç½®ç»Ÿè®¡: min={bias.min().item():.6f}, max={bias.max().item():.6f}, mean={bias.mean().item():.6f}")
        #         print(f"åç½®æ¢¯åº¦ç»Ÿè®¡: {'æœ‰æ¢¯åº¦' if bias.grad is not None else 'æ— æ¢¯åº¦'}")
        #         if bias.grad is not None:
        #             print(f"åç½®æ¢¯åº¦ç»Ÿè®¡: min={bias.grad.min().item():.6f}, max={bias.grad.max().item():.6f}, mean={bias.grad.mean().item():.6f}")
        #             print(f"åç½®æ¢¯åº¦èŒƒæ•°: {bias.grad.norm().item():.6f}")
            
        #     # æ£€æŸ¥æƒé‡æ˜¯å¦åœ¨è®­ç»ƒä¸­å‘ç”Ÿå˜åŒ–
        #     if not hasattr(self, '_prev_weights'):
        #         self._prev_weights = weight.clone().detach()
        #         print("é¦–æ¬¡è®°å½•æƒé‡")
        #     else:
        #         weight_diff = torch.abs(weight - self._prev_weights).mean().item()
        #         weight_max_diff = torch.abs(weight - self._prev_weights).max().item()
        #         print(f"æƒé‡å¹³å‡å˜åŒ–é‡: {weight_diff:.8f}")
        #         print(f"æƒé‡æœ€å¤§å˜åŒ–é‡: {weight_max_diff:.8f}")
        #         if weight_diff < 1e-8:
        #             print("âš ï¸  è­¦å‘Š: æƒé‡å‡ ä¹æ²¡æœ‰å˜åŒ–ï¼Œå¯èƒ½æ²¡æœ‰åœ¨è®­ç»ƒ!")
        #             # è¿›ä¸€æ­¥æ£€æŸ¥ä¼˜åŒ–å™¨çŠ¶æ€
        #             self._check_optimizer_state()
        #         else:
        #             print("âœ… æƒé‡æ­£åœ¨æ›´æ–°")
        #         self._prev_weights = weight.clone().detach()
            
        #     print("=" * 60 + "\n")
        # else:
        #     print(f"\nâš ï¸  {stage_name}é˜¶æ®µç»“æŸ - åˆ†ç±»å¤´å°šæœªåˆ›å»º (Epoch {self.current_epoch})\n")
        pass

    def _check_optimizer_state(self):
        """æ£€æŸ¥ä¼˜åŒ–å™¨çŠ¶æ€ä»¥è¯Šæ–­è®­ç»ƒé—®é¢˜"""
        if hasattr(self, 'optimizer'):
            # print("\n=== ä¼˜åŒ–å™¨çŠ¶æ€è¯Šæ–­ ===")
            
            # æ£€æŸ¥å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            # print(f"å½“å‰å­¦ä¹ ç‡: {current_lr}")
            
            if current_lr == 0:
                # print("âŒ å­¦ä¹ ç‡ä¸º0ï¼Œè¿™ä¼šé˜»æ­¢å‚æ•°æ›´æ–°!")
                pass
            elif current_lr < 1e-8:
                # print("âš ï¸  å­¦ä¹ ç‡éå¸¸å°ï¼Œå¯èƒ½å¯¼è‡´ç¼“æ…¢çš„æ”¶æ•›")
                pass
            
            # æ£€æŸ¥åˆ†ç±»å¤´å‚æ•°æ˜¯å¦åœ¨ä¼˜åŒ–å™¨ä¸­
            classification_head_param_ids = {id(p) for p in self.classification_head.parameters()}
            optimizer_param_ids = set()
            for param_group in self.optimizer.param_groups:
                for param in param_group['params']:
                    optimizer_param_ids.add(id(param))
            
            missing_params = classification_head_param_ids - optimizer_param_ids
            if missing_params:
                # print("âŒ åˆ†ç±»å¤´å‚æ•°ä¸åœ¨ä¼˜åŒ–å™¨ä¸­!")
                pass
            else:
                # print("âœ… åˆ†ç±»å¤´å‚æ•°å·²åœ¨ä¼˜åŒ–å™¨ä¸­")
                pass
            
            # æ£€æŸ¥æ¢¯åº¦
            total_grad_norm = 0.0
            param_count = 0
            for param_group in self.optimizer.param_groups:
                for param in param_group['params']:
                    if param.grad is not None:
                        total_grad_norm += param.grad.norm().item() ** 2
                        param_count += 1
            
            if param_count > 0:
                total_grad_norm = total_grad_norm ** 0.5
                # print(f"æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}")
                # print(f"æœ‰æ¢¯åº¦çš„å‚æ•°æ•°: {param_count}")
            else:
                # print("âŒ æ²¡æœ‰å‚æ•°æœ‰æ¢¯åº¦!")
                pass
            
            # print("=" * 30)
        pass

    def _verify_classification_head_in_optimizer(self):
        """éªŒè¯åˆ†ç±»å¤´å‚æ•°æ˜¯å¦åŒ…å«åœ¨ä¼˜åŒ–å™¨ä¸­"""
        # if hasattr(self, 'classification_head') and hasattr(self, 'optimizer'):
        #     # è·å–åˆ†ç±»å¤´å‚æ•°çš„id
        #     classification_head_param_ids = {id(p) for p in self.classification_head.parameters()}
            
        #     # è·å–ä¼˜åŒ–å™¨ä¸­æ‰€æœ‰å‚æ•°çš„id
        #     optimizer_param_ids = set()
        #     for param_group in self.optimizer.param_groups:
        #         for param in param_group['params']:
        #             optimizer_param_ids.add(id(param))
            
        #     # æ£€æŸ¥äº¤é›†
        #     included_params = classification_head_param_ids & optimizer_param_ids
        #     missing_params = classification_head_param_ids - optimizer_param_ids
            
        #     print(f"\n=== åˆ†ç±»å¤´å‚æ•°ä¼˜åŒ–å™¨éªŒè¯ ===")
        #     print(f"åˆ†ç±»å¤´æ€»å‚æ•°æ•°: {len(classification_head_param_ids)}")
        #     print(f"ä¼˜åŒ–å™¨ä¸­çš„åˆ†ç±»å¤´å‚æ•°æ•°: {len(included_params)}")
        #     print(f"ç¼ºå¤±çš„åˆ†ç±»å¤´å‚æ•°æ•°: {len(missing_params)}")
            
        #     if missing_params:
        #         print("âŒ è­¦å‘Š: ä»¥ä¸‹åˆ†ç±»å¤´å‚æ•°æœªåŒ…å«åœ¨ä¼˜åŒ–å™¨ä¸­:")
        #         for name, param in self.classification_head.named_parameters():
        #             if id(param) in missing_params:
        #                 print(f"  - {name}: {param.shape}, requires_grad={param.requires_grad}")
        #         print("ğŸ”§ æ­£åœ¨é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨...")
        #         self.init_optimizers()
        #         print("âœ… ä¼˜åŒ–å™¨é‡æ–°åˆå§‹åŒ–å®Œæˆ")
        #     else:
        #         print("âœ… æ‰€æœ‰åˆ†ç±»å¤´å‚æ•°éƒ½å·²åŒ…å«åœ¨ä¼˜åŒ–å™¨ä¸­")
            
        #     # éªŒè¯å‚æ•°çš„requires_gradè®¾ç½®
        #     print(f"\n=== åˆ†ç±»å¤´å‚æ•°æ¢¯åº¦è®¾ç½® ===")
        #     for name, param in self.classification_head.named_parameters():
        #         print(f"{name}: shape={param.shape}, requires_grad={param.requires_grad}, device={param.device}")
            
        #     print("=" * 50 + "\n")
        pass

    def init_optimizers(self):
        """é‡å†™ä¼˜åŒ–å™¨åˆå§‹åŒ–ï¼Œç¡®ä¿åŒ…å«åˆ†ç±»å¤´å‚æ•°"""
        import copy
        copy_optimizer_kwargs = copy.deepcopy(self.optimizer_kwargs)
        
        # No decay for layer norm and bias
        no_decay = ['LayerNorm.weight', 'bias']
        weight_decay = copy_optimizer_kwargs.pop("weight_decay")

        # æ”¶é›†æ‰€æœ‰éœ€è¦ä¼˜åŒ–çš„å‚æ•°
        all_params = []
        esm3_param_count = 0
        
        # æ·»åŠ ESM3æ¨¡å‹å‚æ•°
        if hasattr(self, 'model') and self.model is not None:
            for name, param in self.model.named_parameters():
                if param.requires_grad:
                    all_params.append((name, param))
                    esm3_param_count += 1
        
        # print(f"ESM3æ¨¡å‹å¯è®­ç»ƒå‚æ•°æ•°é‡: {esm3_param_count}")
        
        # æ·»åŠ åˆ†ç±»å¤´å‚æ•°
        classification_head_param_count = 0
        if hasattr(self, 'classification_head') and self.classification_head is not None:
            # print(f"ğŸ” æ£€æŸ¥åˆ†ç±»å¤´å‚æ•°...")
            # print(f"åˆ†ç±»å¤´ç±»å‹: {type(self.classification_head)}")
            # print(f"åˆ†ç±»å¤´è®¾å¤‡: {next(self.classification_head.parameters()).device if list(self.classification_head.parameters()) else 'N/A'}")
            
            for name, param in self.classification_head.named_parameters():
                # print(f"  å‚æ•°: {name}, shape={param.shape}, requires_grad={param.requires_grad}, device={param.device}")
                if param.requires_grad:
                    full_name = f"classification_head.{name}"
                    all_params.append((full_name, param))
                    classification_head_param_count += 1
                    # print(f"  âœ… æ·»åŠ åˆ°ä¼˜åŒ–å™¨: {full_name}")
                # else:
                #     print(f"  âŒ è·³è¿‡ï¼ˆrequires_grad=Falseï¼‰: {name}")
        # else:
        #     print("âŒ åˆ†ç±»å¤´ä¸å­˜åœ¨æˆ–ä¸ºNone")

        # print(f"åˆ†ç±»å¤´å¯è®­ç»ƒå‚æ•°æ•°é‡: {classification_head_param_count}")
        # print(f"æ€»å¯è®­ç»ƒå‚æ•°æ•°é‡: {len(all_params)}")

        if not all_params:
            # print("âš ï¸ è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°éœ€è¦ä¼˜åŒ–çš„å‚æ•°!")
            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿå‚æ•°é¿å…ä¼˜åŒ–å™¨é”™è¯¯
            dummy_param = torch.nn.Parameter(torch.tensor(0.0))
            optimizer_grouped_parameters = [
                {'params': [dummy_param], 'weight_decay': 0.0}
            ]
        else:
            # æ ¹æ®å‚æ•°åç§°åˆ†ç»„
            optimizer_grouped_parameters = [
                {'params': [param for name, param in all_params if not any(nd in name for nd in no_decay)],
                 'weight_decay': weight_decay},
                {'params': [param for name, param in all_params if any(nd in name for nd in no_decay)],
                 'weight_decay': 0.0}
            ]
            
            # print(f"âœ… ä¼˜åŒ–å™¨å‚æ•°åˆ†ç»„:")
            # print(f"  - å¸¦æƒé‡è¡°å‡çš„å‚æ•°: {len(optimizer_grouped_parameters[0]['params'])}")
            # print(f"  - ä¸å¸¦æƒé‡è¡°å‡çš„å‚æ•°: {len(optimizer_grouped_parameters[1]['params'])}")

        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer_cls = eval(f"torch.optim.{copy_optimizer_kwargs.pop('class')}")
        self.optimizer = optimizer_cls(optimizer_grouped_parameters,
                                       lr=self.lr_scheduler_kwargs['init_lr'],
                                       **copy_optimizer_kwargs)
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        tmp_kwargs = copy.deepcopy(self.lr_scheduler_kwargs)
        lr_scheduler_name = tmp_kwargs.pop("class")
        
        # æ ¹æ®è°ƒåº¦å™¨åç§°é€‰æ‹©æ­£ç¡®çš„ç±»
        if lr_scheduler_name == "ConstantLRScheduler":
            lr_scheduler_cls = ConstantLRScheduler
        elif lr_scheduler_name == "CosineAnnealingLRScheduler":
            lr_scheduler_cls = CosineAnnealingLRScheduler
        elif lr_scheduler_name == "Esm2LRScheduler":
            lr_scheduler_cls = Esm2LRScheduler
        elif hasattr(torch.optim.lr_scheduler, lr_scheduler_name):
            # å¦‚æœæ˜¯PyTorchå†…ç½®çš„è°ƒåº¦å™¨
            lr_scheduler_cls = getattr(torch.optim.lr_scheduler, lr_scheduler_name)
        else:
            # print(f"âš ï¸  æœªçŸ¥çš„å­¦ä¹ ç‡è°ƒåº¦å™¨: {lr_scheduler_name}, ä½¿ç”¨ConstantLRScheduler")
            lr_scheduler_cls = ConstantLRScheduler
            
        self.lr_scheduler = lr_scheduler_cls(self.optimizer, **tmp_kwargs)
        
        # print(f"âœ… ä¼˜åŒ–å™¨é‡æ–°åˆå§‹åŒ–å®Œæˆï¼Œæ€»å‚æ•°ç»„æ•°: {len(optimizer_grouped_parameters)}")
        # print(f"âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨: {lr_scheduler_name}")
        # print(f"âœ… åˆå§‹å­¦ä¹ ç‡: {self.lr_scheduler_kwargs.get('init_lr', 'N/A')}")

    def training_step(self, batch, batch_idx):
        """é‡å†™è®­ç»ƒæ­¥éª¤ï¼Œæ·»åŠ è¯¦ç»†çš„æ¢¯åº¦ç›‘æ§"""
        inputs, labels = batch
        
        # åœ¨å‰å‘ä¼ æ’­å‰æ£€æŸ¥å‚æ•°æ¢¯åº¦çŠ¶æ€
        # if batch_idx == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªbatchæ—¶æ‰“å°
        #     print(f"\nğŸ” è®­ç»ƒæ­¥éª¤ {batch_idx} å¼€å§‹å‰çš„å‚æ•°çŠ¶æ€:")
        #     for name, param in self.classification_head.named_parameters():
        #         print(f"  {name}: requires_grad={param.requires_grad}, grad={'æœ‰' if param.grad is not None else 'æ— '}")
        
        # å‰å‘ä¼ æ’­
        outputs = self(**inputs)
        
        # è®¡ç®—æŸå¤±
        loss = self.loss_func('train', outputs, labels)
        
        # print(f"ğŸ” Batch {batch_idx}: Loss = {loss.item():.6f}")
        
        # åœ¨è¿”å›lossä¹‹å‰æ£€æŸ¥æ¢¯åº¦ï¼ˆPyTorch Lightningä¼šè‡ªåŠ¨è°ƒç”¨backwardï¼‰
        # if batch_idx == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªbatchæ—¶æ‰“å°
        #     print(f"ğŸ” æŸå¤±è®¡ç®—å®Œæˆï¼Œå‡†å¤‡åå‘ä¼ æ’­...")
        #     print(f"  Loss requires_grad: {loss.requires_grad}")
        #     print(f"  Loss grad_fn: {loss.grad_fn}")
        
        self.log("loss", loss, prog_bar=True)
        return loss

    def on_before_optimizer_step(self, optimizer):
        """åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹å‰æ£€æŸ¥æ¢¯åº¦"""
        # æ£€æŸ¥åˆ†ç±»å¤´æ¢¯åº¦
        # total_grad_norm = 0.0
        # param_count = 0
        
        # print(f"\nğŸ” ä¼˜åŒ–å™¨æ­¥éª¤å‰çš„æ¢¯åº¦æ£€æŸ¥:")
        # for name, param in self.classification_head.named_parameters():
        #     if param.grad is not None:
        #         grad_norm = param.grad.norm().item()
        #         total_grad_norm += grad_norm ** 2
        #         param_count += 1
        #         print(f"  {name}: grad_norm={grad_norm:.6f}")
        #     else:
        #         print(f"  {name}: âŒ æ— æ¢¯åº¦!")
        
        # if param_count > 0:
        #     total_grad_norm = total_grad_norm ** 0.5
        #     print(f"  åˆ†ç±»å¤´æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}")
        # else:
        #     print(f"  âŒ åˆ†ç±»å¤´æ²¡æœ‰ä»»ä½•å‚æ•°æœ‰æ¢¯åº¦!")
        
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        super().on_before_optimizer_step(optimizer)