import torchmetrics
import torch
import torch.distributed as dist

from torch.nn import Linear, ReLU
from torch.nn.functional import cross_entropy
from ..model_interface import register_model
from .base import SaprotBaseModel
# å¯¼å…¥å­¦ä¹ çŽ‡è°ƒåº¦å™¨ - ä¿®å¤å¯¼å…¥è·¯å¾„
from utils.lr_scheduler import ConstantLRScheduler, CosineAnnealingLRScheduler, Esm2LRScheduler


@register_model
class SaprotPairRegressionModel(SaprotBaseModel):
    def __init__(self, fixed_seq_length: int = 2048, optimizer_kwargs=None, lr_scheduler_kwargs=None, **kwargs):
        """
        Args:
            fixed_seq_length: å›ºå®šåºåˆ—é•¿åº¦ï¼Œç”¨äºŽæˆªæ–­æˆ–padding
            optimizer_kwargs: ä¼˜åŒ–å™¨å‚æ•°
            lr_scheduler_kwargs: å­¦ä¹ çŽ‡è°ƒåº¦å™¨å‚æ•°
            **kwargs: other arguments for SaprotBaseModel
        """
        # è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ çŽ‡è°ƒåº¦å™¨å‚æ•°
        self.optimizer_kwargs = optimizer_kwargs or {
            "class": "AdamW",
            "weight_decay": 0.01,
            "betas": (0.9, 0.999),
            "eps": 1e-8
        }
        
        self.lr_scheduler_kwargs = lr_scheduler_kwargs or {
            "class": "ConstantLRScheduler",
            "init_lr": 1e-4,
            "num_warmup_steps": 0,
            "num_training_steps": 1000
        }
        
        self.fixed_seq_length = fixed_seq_length
        super().__init__(task="base", **kwargs)
        
        # å›žå½’å¤´å°†åœ¨initialize_modelä¸­åˆ›å»º
        # print(f"å›žå½’å¤´å°†åœ¨initialize_modelä¸­åˆ›å»º")
        
        # åˆ›å»ºå›ºå®šç»´åº¦çš„å›žå½’å¤´
        self.regression_head = torch.nn.Linear(self.fixed_seq_length * 2, 1)  # *2 for pair
        
        # print(f"åˆ›å»ºå›ºå®špairå›žå½’å¤´: {self.fixed_seq_length * 2} -> 1")
        # print(f"å›žå½’å¤´å‚æ•°: weight={self.regression_head.weight.shape}, bias={self.regression_head.bias.shape}")
        
        # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨ä»¥åŒ…å«å›žå½’å¤´å‚æ•°
        self.init_optimizers()

    def initialize_model(self):
        super().initialize_model()

        # ä¿ç•™åŽŸæœ‰çš„classifierä½œä¸ºå…œåº•
        hidden_size = self.model.config.hidden_size * 2
        classifier = torch.nn.Sequential(
            Linear(hidden_size, hidden_size),
            ReLU(),
            Linear(hidden_size, 1)
        )

        setattr(self.model, "classifier", classifier)

    def initialize_metrics(self, stage):
        return {f"{stage}_loss": torchmetrics.MeanSquaredError(),
                f"{stage}_spearman": torchmetrics.SpearmanCorrCoef(),
                f"{stage}_R2": torchmetrics.R2Score(),
                f"{stage}_pearson": torchmetrics.PearsonCorrCoef()}

    def setup(self, stage=None):
        """PyTorch Lightningçš„setupæ–¹æ³•ï¼Œåœ¨è¿™é‡Œè®¾ç½®ESM3æ¨¡åž‹åˆ°æ•°æ®é›†"""
        super().setup(stage)
        # print("pairå›žå½’æ¨¡åž‹setupå®Œæˆï¼Œå°†åœ¨è®­ç»ƒå¼€å§‹æ—¶è®¾ç½®ESM3æ¨¡åž‹åˆ°æ•°æ®é›†")

    def on_train_start(self):
        """è®­ç»ƒå¼€å§‹æ—¶çš„å›žè°ƒï¼Œç¡®ä¿ESM3æ¨¡åž‹ä¼ é€’ç»™æ•°æ®é›†"""
        super().on_train_start()
        self._set_esm_model_to_datasets()

    def on_validation_start(self):
        """éªŒè¯å¼€å§‹æ—¶çš„å›žè°ƒï¼Œç¡®ä¿ESM3æ¨¡åž‹ä¼ é€’ç»™æ•°æ®é›†"""
        super().on_validation_start()
        self._set_esm_model_to_datasets()

    def on_test_start(self):
        """æµ‹è¯•å¼€å§‹æ—¶çš„å›žè°ƒï¼Œç¡®ä¿ESM3æ¨¡åž‹ä¼ é€’ç»™æ•°æ®é›†"""
        super().on_test_start()
        self._set_esm_model_to_datasets()

    def _set_esm_model_to_datasets(self):
        """å°†ESM3æ¨¡åž‹è®¾ç½®åˆ°æ‰€æœ‰æ•°æ®é›†"""
        if hasattr(self.trainer, 'datamodule'):
            datasets = []
            
            # èŽ·å–æ‰€æœ‰æ•°æ®é›†å®žä¾‹
            if hasattr(self.trainer.datamodule, 'train_dataset'):
                datasets.append(('train', self.trainer.datamodule.train_dataset))
            if hasattr(self.trainer.datamodule, 'val_dataset'):
                datasets.append(('val', self.trainer.datamodule.val_dataset))
            if hasattr(self.trainer.datamodule, 'test_dataset'):
                datasets.append(('test', self.trainer.datamodule.test_dataset))
            
            # è®¾ç½®ESM3æ¨¡åž‹
            for stage, dataset in datasets:
                if dataset is not None and hasattr(dataset, 'set_esm_model'):
                    # print(f"è®¾ç½®ESM3æ¨¡åž‹åˆ°{stage}æ•°æ®é›†: {type(dataset).__name__}")
                    dataset.set_esm_model(self.model)
                    
            # å¦å¤–æ£€æŸ¥dataloaderä¸­çš„æ•°æ®é›†
            dataloaders = []
            
            # å®‰å…¨åœ°èŽ·å–dataloaders
            if hasattr(self.trainer, 'train_dataloader') and self.trainer.train_dataloader is not None:
                train_dl = self.trainer.train_dataloader
                if callable(train_dl):
                    train_dl = train_dl()
                dataloaders.append(('train', train_dl))
                
            if hasattr(self.trainer, 'val_dataloaders') and self.trainer.val_dataloaders is not None:
                val_dl = self.trainer.val_dataloaders
                if callable(val_dl):
                    val_dl = val_dl()
                if isinstance(val_dl, list):
                    for i, dl in enumerate(val_dl):
                        dataloaders.append((f'val_{i}', dl))
                else:
                    dataloaders.append(('val', val_dl))
                    
            if hasattr(self.trainer, 'test_dataloaders') and self.trainer.test_dataloaders is not None:
                test_dl = self.trainer.test_dataloaders
                if callable(test_dl):
                    test_dl = test_dl()
                if isinstance(test_dl, list):
                    for i, dl in enumerate(test_dl):
                        dataloaders.append((f'test_{i}', dl))
                else:
                    dataloaders.append(('test', test_dl))
            
            for stage, dataloader in dataloaders:
                if dataloader is not None:
                    if hasattr(dataloader, 'dataset') and hasattr(dataloader.dataset, 'set_esm_model'):
                        # print(f"è®¾ç½®ESM3æ¨¡åž‹åˆ°{stage} dataloaderæ•°æ®é›†: {type(dataloader.dataset).__name__}")
                        dataloader.dataset.set_esm_model(self.model)

    def _pad_or_truncate_features(self, features, target_length):
        """
        å°†ç‰¹å¾æˆªæ–­æˆ–paddingåˆ°å›ºå®šé•¿åº¦
        Args:
            features: è¾“å…¥ç‰¹å¾ tensor [batch_size, seq_len] æˆ– [batch_size, seq_len, hidden_dim]
            target_length: ç›®æ ‡é•¿åº¦
        Returns:
            å¤„ç†åŽçš„ç‰¹å¾ [batch_size, target_length] æˆ– [batch_size, target_length, hidden_dim]
        """
        if features.dim() == 2:
            # [batch_size, seq_len] çš„æƒ…å†µ
            batch_size, seq_len = features.shape
            if seq_len > target_length:
                # æˆªæ–­
                return features[:, :target_length]
            elif seq_len < target_length:
                # padding
                padding_size = target_length - seq_len
                padding = torch.zeros(batch_size, padding_size, device=features.device, dtype=features.dtype)
                return torch.cat([features, padding], dim=1)
            else:
                return features
        elif features.dim() == 3:
            # [batch_size, seq_len, hidden_dim] çš„æƒ…å†µï¼Œå…ˆå¹³å‡æ± åŒ–
            features = features.mean(dim=2)  # [batch_size, seq_len]
            return self._pad_or_truncate_features(features, target_length)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç‰¹å¾ç»´åº¦: {features.shape}")

    def forward(self, inputs_1=None, inputs_2=None, sequences_1=None, sequences_2=None, embeddings_1=None, embeddings_2=None, tokens_1=None, tokens_2=None, **kwargs):
        # Handle different input formats
        if inputs_1 is None and sequences_1 is not None:
            inputs_1 = {"sequences": sequences_1}
        elif inputs_1 is None and embeddings_1 is not None:
            inputs_1 = {"embeddings": embeddings_1}
        elif inputs_1 is None and tokens_1 is not None:
            inputs_1 = {"tokens": tokens_1}
        elif inputs_1 is None:
            inputs_1 = kwargs.get('inputs_1', {})
            
        if inputs_2 is None and sequences_2 is not None:
            inputs_2 = {"sequences": sequences_2}
        elif inputs_2 is None and embeddings_2 is not None:
            inputs_2 = {"embeddings": embeddings_2}
        elif inputs_2 is None and tokens_2 is not None:
            inputs_2 = {"tokens": tokens_2}
        elif inputs_2 is None:
            inputs_2 = kwargs.get('inputs_2', {})
        
        # Get device and dtype from model parameters
        device = next(self.model.parameters()).device
        model_dtype = next(self.model.parameters()).dtype
        
        # ä¼˜å…ˆå¤„ç†tokens
        if "tokens" in inputs_1 and "tokens" in inputs_2:
            tokens_1 = inputs_1["tokens"].to(device=device)
            tokens_2 = inputs_2["tokens"].to(device=device)
            
            # å°†tokensè½¬æ¢ä¸ºæµ®ç‚¹æ•°ç±»åž‹å¹¶è¿›è¡Œæˆªæ–­/padding
            try:
                tokens_1_float = tokens_1.float().to(dtype=model_dtype)
                tokens_2_float = tokens_2.float().to(dtype=model_dtype)
                
                if tokens_1_float.dim() == 2 and tokens_2_float.dim() == 2:
                    # æˆªæ–­æˆ–paddingåˆ°å›ºå®šé•¿åº¦
                    features_1 = self._pad_or_truncate_features(tokens_1_float, self.fixed_seq_length)
                    features_2 = self._pad_or_truncate_features(tokens_2_float, self.fixed_seq_length)
                    
                    # è¿žæŽ¥ä¸¤ä¸ªåºåˆ—çš„ç‰¹å¾
                    stacked_features = torch.cat([features_1, features_2], dim=1)
                else:
                    batch_size = tokens_1.shape[0] if tokens_1.dim() > 0 else 1
                    stacked_features = torch.zeros(batch_size, self.fixed_seq_length * 2, device=device, dtype=model_dtype)
                
            except Exception as e:
                batch_size = tokens_1.shape[0] if tokens_1.dim() > 0 else 1
                stacked_features = torch.zeros(batch_size, self.fixed_seq_length * 2, device=device, dtype=model_dtype)
        
        # å¤„ç†é¢„ç¼–ç çš„åµŒå…¥
        elif "embeddings" in inputs_1 and "embeddings" in inputs_2:
            embeddings_1 = inputs_1["embeddings"].to(device=device, dtype=model_dtype)
            embeddings_2 = inputs_2["embeddings"].to(device=device, dtype=model_dtype)
            
            # å¦‚æžœæ˜¯é«˜ç»´åµŒå…¥ï¼Œéœ€è¦è½¬æ¢ä¸ºå›ºå®šé•¿åº¦
            if embeddings_1.dim() == 3:
                embeddings_1 = embeddings_1.mean(dim=2)
            if embeddings_2.dim() == 3:
                embeddings_2 = embeddings_2.mean(dim=2)
                
            features_1 = self._pad_or_truncate_features(embeddings_1, self.fixed_seq_length)
            features_2 = self._pad_or_truncate_features(embeddings_2, self.fixed_seq_length)
            stacked_features = torch.cat([features_1, features_2], dim=1)
        
        elif "sequences" in inputs_1 and "sequences" in inputs_2:
            sequences_1 = inputs_1["sequences"]
            sequences_2 = inputs_2["sequences"]
            
            # Process sequences using ESM3 in the model
            from esm.sdk.api import ESMProtein
            
            features_1 = []
            features_2 = []
            
            for i, (seq_1, seq_2) in enumerate(zip(sequences_1, sequences_2)):
                try:
                    # ç¼–ç ç¬¬ä¸€ä¸ªåºåˆ—
                    protein_1 = ESMProtein(sequence=seq_1)
                    with torch.no_grad():
                        encoded_protein_1 = self.model.encode(protein_1)
                    
                    # ç¼–ç ç¬¬äºŒä¸ªåºåˆ—
                    protein_2 = ESMProtein(sequence=seq_2)
                    with torch.no_grad():
                        encoded_protein_2 = self.model.encode(protein_2)
                    
                    # æå–sequence tokens
                    if hasattr(encoded_protein_1, 'sequence') and hasattr(encoded_protein_2, 'sequence'):
                        seq_tokens_1 = getattr(encoded_protein_1, 'sequence')
                        seq_tokens_2 = getattr(encoded_protein_2, 'sequence')
                        
                        if torch.is_tensor(seq_tokens_1) and torch.is_tensor(seq_tokens_2):
                            # ç›´æŽ¥ä½¿ç”¨tokensä½œä¸ºç‰¹å¾
                            seq_feature_1 = seq_tokens_1.float()
                            seq_feature_2 = seq_tokens_2.float()
                            
                            # æˆªæ–­æˆ–paddingåˆ°å›ºå®šé•¿åº¦
                            if len(seq_feature_1) > self.fixed_seq_length:
                                seq_feature_1 = seq_feature_1[:self.fixed_seq_length]
                            elif len(seq_feature_1) < self.fixed_seq_length:
                                padding_size = self.fixed_seq_length - len(seq_feature_1)
                                padding = torch.zeros(padding_size, device=device, dtype=model_dtype)
                                seq_feature_1 = torch.cat([seq_feature_1, padding])
                            
                            if len(seq_feature_2) > self.fixed_seq_length:
                                seq_feature_2 = seq_feature_2[:self.fixed_seq_length]
                            elif len(seq_feature_2) < self.fixed_seq_length:
                                padding_size = self.fixed_seq_length - len(seq_feature_2)
                                padding = torch.zeros(padding_size, device=device, dtype=model_dtype)
                                seq_feature_2 = torch.cat([seq_feature_2, padding])
                            
                            features_1.append(seq_feature_1.to(device=device, dtype=model_dtype))
                            features_2.append(seq_feature_2.to(device=device, dtype=model_dtype))
                        else:
                            # åˆ›å»ºé›¶å‘é‡
                            feature_1 = torch.zeros(self.fixed_seq_length, device=device, dtype=model_dtype)
                            feature_2 = torch.zeros(self.fixed_seq_length, device=device, dtype=model_dtype)
                            features_1.append(feature_1)
                            features_2.append(feature_2)
                    else:
                        # åˆ›å»ºé›¶å‘é‡
                        feature_1 = torch.zeros(self.fixed_seq_length, device=device, dtype=model_dtype)
                        feature_2 = torch.zeros(self.fixed_seq_length, device=device, dtype=model_dtype)
                        features_1.append(feature_1)
                        features_2.append(feature_2)
                except Exception as e:
                    feature_1 = torch.zeros(self.fixed_seq_length, device=device, dtype=model_dtype)
                    feature_2 = torch.zeros(self.fixed_seq_length, device=device, dtype=model_dtype)
                    features_1.append(feature_1)
                    features_2.append(feature_2)
            
            if features_1 and features_2:
                stacked_features_1 = torch.stack(features_1)
                stacked_features_2 = torch.stack(features_2)
                stacked_features = torch.cat([stacked_features_1, stacked_features_2], dim=1)
            else:
                stacked_features = torch.zeros(1, self.fixed_seq_length * 2, device=device, dtype=model_dtype)
        
        # ä¿ç•™åŽŸæœ‰çš„ESMå’ŒProtBERTé€»è¾‘ä½œä¸ºå…œåº•
        elif "inputs" in inputs_1 and "inputs" in inputs_2:
            model_inputs_1 = inputs_1["inputs"]
            model_inputs_2 = inputs_2["inputs"]
            
            if self.freeze_backbone:
                hidden_1 = torch.stack(self.get_hidden_states_from_dict(model_inputs_1, reduction="mean"))
                hidden_2 = torch.stack(self.get_hidden_states_from_dict(model_inputs_2, reduction="mean"))
            else:
                # If "esm" is not in the model, use "bert" as the backbone
                backbone = self.model.esm if hasattr(self.model, "esm") else self.model.bert
                hidden_1 = backbone(**model_inputs_1)[0][:, 0, :]
                hidden_2 = backbone(**model_inputs_2)[0][:, 0, :]
            
            hidden_concat = torch.cat([hidden_1, hidden_2], dim=-1)
            return self.model.classifier(hidden_concat).squeeze(dim=-1)
        
        else:
            stacked_features = torch.zeros(1, self.fixed_seq_length * 2, device=device, dtype=model_dtype)
        
        # Ensure stacked_features is on the correct device and dtype
        stacked_features = stacked_features.to(device=device, dtype=model_dtype)
        
        # ç¡®ä¿å›žå½’å¤´åœ¨æ­£ç¡®çš„è®¾å¤‡å’Œæ•°æ®ç±»åž‹ä¸Š
        self.regression_head = self.regression_head.to(device=device, dtype=model_dtype)
        
        # Forward pass
        logits = self.regression_head(stacked_features).squeeze(dim=-1)
        
        return logits

    def loss_func(self, stage, logits, labels):
        fitness = labels['labels'].to(logits)
        loss = torch.nn.functional.mse_loss(logits, fitness)

        # Update metrics
        for metric in self.metrics[stage].values():
            # Training is on half precision, but metrics expect float to compute correctly.
            metric.update(logits.detach().float(), fitness.float())

        if stage == "train":
            log_dict = {"train_loss": loss.item()}
            self.log_info(log_dict)

            # Reset train metrics
            self.reset_metrics("train")

        return loss

    def init_optimizers(self):
        """é‡å†™ä¼˜åŒ–å™¨åˆå§‹åŒ–ï¼Œç¡®ä¿åŒ…å«å›žå½’å¤´å‚æ•°"""
        import copy
        copy_optimizer_kwargs = copy.deepcopy(self.optimizer_kwargs)
        
        # No decay for layer norm and bias
        no_decay = ['LayerNorm.weight', 'bias']
        weight_decay = copy_optimizer_kwargs.pop("weight_decay")

        # æ”¶é›†æ‰€æœ‰éœ€è¦ä¼˜åŒ–çš„å‚æ•°
        all_params = []
        esm3_param_count = 0
        
        # æ·»åŠ ESM3æ¨¡åž‹å‚æ•°
        if hasattr(self, 'model') and self.model is not None:
            for name, param in self.model.named_parameters():
                if param.requires_grad:
                    all_params.append((name, param))
                    esm3_param_count += 1
        
        # print(f"ESM3æ¨¡åž‹å¯è®­ç»ƒå‚æ•°æ•°é‡: {esm3_param_count}")
        
        # æ·»åŠ å›žå½’å¤´å‚æ•°
        regression_head_param_count = 0
        if hasattr(self, 'regression_head') and self.regression_head is not None:
            for name, param in self.regression_head.named_parameters():
                if param.requires_grad:
                    full_name = f"regression_head.{name}"
                    all_params.append((full_name, param))
                    regression_head_param_count += 1
                    # print(f"  âœ… æ·»åŠ åˆ°ä¼˜åŒ–å™¨: {full_name}")

        # print(f"å›žå½’å¤´å¯è®­ç»ƒå‚æ•°æ•°é‡: {regression_head_param_count}")
        # print(f"æ€»å¯è®­ç»ƒå‚æ•°æ•°é‡: {len(all_params)}")

        if not all_params:
            # print("âš ï¸ è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°éœ€è¦ä¼˜åŒ–çš„å‚æ•°!")
            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿå‚æ•°é¿å…ä¼˜åŒ–å™¨é”™è¯¯
            dummy_param = torch.nn.Parameter(torch.tensor(0.0))
            optimizer_grouped_parameters = [
                {'params': [dummy_param], 'weight_decay': 0.0}
            ]
        else:
            # æ ¹æ®å‚æ•°åç§°åˆ†ç»„
            optimizer_grouped_parameters = [
                {'params': [param for name, param in all_params if not any(nd in name for nd in no_decay)],
                 'weight_decay': weight_decay},
                {'params': [param for name, param in all_params if any(nd in name for nd in no_decay)],
                 'weight_decay': 0.0}
            ]

        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer_cls = eval(f"torch.optim.{copy_optimizer_kwargs.pop('class')}")
        self.optimizer = optimizer_cls(optimizer_grouped_parameters,
                                       lr=self.lr_scheduler_kwargs['init_lr'],
                                       **copy_optimizer_kwargs)
        
        # åˆ›å»ºå­¦ä¹ çŽ‡è°ƒåº¦å™¨
        tmp_kwargs = copy.deepcopy(self.lr_scheduler_kwargs)
        lr_scheduler_name = tmp_kwargs.pop("class")
        
        # æ ¹æ®è°ƒåº¦å™¨åç§°é€‰æ‹©æ­£ç¡®çš„ç±»
        if lr_scheduler_name == "ConstantLRScheduler":
            lr_scheduler_cls = ConstantLRScheduler
        elif lr_scheduler_name == "CosineAnnealingLRScheduler":
            lr_scheduler_cls = CosineAnnealingLRScheduler
        elif lr_scheduler_name == "Esm2LRScheduler":
            lr_scheduler_cls = Esm2LRScheduler
        elif hasattr(torch.optim.lr_scheduler, lr_scheduler_name):
            # å¦‚æžœæ˜¯PyTorchå†…ç½®çš„è°ƒåº¦å™¨
            lr_scheduler_cls = getattr(torch.optim.lr_scheduler, lr_scheduler_name)
        else:
            # print(f"âš ï¸  æœªçŸ¥çš„å­¦ä¹ çŽ‡è°ƒåº¦å™¨: {lr_scheduler_name}, ä½¿ç”¨ConstantLRScheduler")
            lr_scheduler_cls = ConstantLRScheduler
            
        self.lr_scheduler = lr_scheduler_cls(self.optimizer, **tmp_kwargs)

    def training_step(self, batch, batch_idx):
        """é‡å†™è®­ç»ƒæ­¥éª¤ï¼Œæ·»åŠ è¯¦ç»†çš„æ¢¯åº¦ç›‘æŽ§"""
        inputs, labels = batch
        
        # å‰å‘ä¼ æ’­
        outputs = self(**inputs)
        
        # è®¡ç®—æŸå¤±
        loss = self.loss_func('train', outputs, labels)
        
        # print(f"ðŸ” Batch {batch_idx}: Loss = {loss.item():.6f}")
        
        self.log("loss", loss, prog_bar=True)
        return loss

    def on_before_optimizer_step(self, optimizer):
        """åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹å‰æ£€æŸ¥æ¢¯åº¦"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        super().on_before_optimizer_step(optimizer)

    def on_test_epoch_end(self):
        log_dict = self.get_log_dict("test")

        # if dist.get_rank() == 0:
        #     print(log_dict)
        self.output_test_metrics(log_dict)
        self.log_info(log_dict)
        self.reset_metrics("test")

    def on_validation_epoch_end(self):
        log_dict = self.get_log_dict("valid")

        # if dist.get_rank() == 0:
        #     print(log_dict)
        self.log_info(log_dict)
        self.reset_metrics("valid")
        self.check_save_condition(log_dict["valid_loss"], mode="min")

        self.plot_valid_metrics_curve(log_dict)