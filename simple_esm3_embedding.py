import torch
import numpy as np
import pandas as pd
from esm.models.esm3 import ESM3
from esm.sdk.api import ESMProtein

# ç®€å•çš„PDBå¤„ç†
def get_sequence_from_pdb(pdb_file_path):
    """ä»PDBæ–‡ä»¶æå–åºåˆ—å’Œåæ ‡"""
    try:
        from Bio.PDB import PDBParser
        
        # æ°¨åŸºé…¸æ˜ å°„
        aa_dict = {
            'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E', 'PHE': 'F',
            'GLY': 'G', 'HIS': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L',
            'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN': 'Q', 'ARG': 'R',
            'SER': 'S', 'THR': 'T', 'VAL': 'V', 'TRP': 'W', 'TYR': 'Y'
        }
        
        parser = PDBParser(QUIET=True)
        structure = parser.get_structure('protein', pdb_file_path)
        
        sequence = ""
        coordinates = []
        
        # å–ç¬¬ä¸€ä¸ªæ¨¡å‹çš„ç¬¬ä¸€æ¡é“¾
        chain = list(structure[0].get_chains())[0]
        
        for residue in chain:
            if residue.id[0] == ' ':  # æ ‡å‡†æ®‹åŸº
                resname = residue.get_resname()
                if resname in aa_dict:
                    sequence += aa_dict[resname]
                    
                    # è·å–CAåŸå­åæ ‡
                    if 'CA' in residue:
                        coordinates.append(residue['CA'].get_coord())
        
        coordinates = np.array(coordinates)
        
        print(f"âœ… åºåˆ—é•¿åº¦: {len(sequence)}")
        print(f"âœ… åæ ‡å½¢çŠ¶: {coordinates.shape}")
        print(f"âœ… åºåˆ—: {sequence[:50]}...")
        
        return sequence, coordinates
        
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…BioPython: pip install biopython")
        return None, None
    except Exception as e:
        print(f"âŒ å¤„ç†PDBæ–‡ä»¶å¤±è´¥: {str(e)}")
        return None, None

def get_esm3_embedding(sequence):
    """è·å–ESM3 embedding - æ”¹è¿›ç‰ˆæœ¬"""
    print(f"ğŸ§¬ è·å–åºåˆ—embedding...")
    
    # åŠ è½½æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = ESM3.from_pretrained("esm3-open").to(device).eval()
    
    # åˆ›å»ºè›‹ç™½è´¨å¯¹è±¡
    protein = ESMProtein(sequence=sequence)
    
    with torch.no_grad():
        # æ–¹æ³•1: ä½¿ç”¨ESM3InferenceClient
        print("ğŸ”¸ å°è¯•æ–¹æ³•1: ä½¿ç”¨ESM3InferenceClient...")
        try:
            from esm.sdk.api import ESM3InferenceClient
            
            # åˆ›å»ºæ¨ç†å®¢æˆ·ç«¯
            client = ESM3InferenceClient(model)
            
            # ä½¿ç”¨å®¢æˆ·ç«¯è¿›è¡Œæ¨ç†
            result = client.encode(protein)
            
            if result is not None:
                print(f"âœ… å®¢æˆ·ç«¯ç»“æœç±»å‹: {type(result)}")
                
                # æ£€æŸ¥ç»“æœçš„å±æ€§
                for attr_name in dir(result):
                    if not attr_name.startswith('_'):
                        try:
                            attr_value = getattr(result, attr_name)
                            if hasattr(attr_value, 'shape') and len(attr_value.shape) >= 2:
                                print(f"âœ… æ‰¾åˆ°embedding: {attr_name} {attr_value.shape}")
                                
                                # ç”Ÿæˆåºåˆ—çº§åˆ«embedding
                                if len(attr_value.shape) == 3:  # [batch, seq, hidden]
                                    content_embedding = attr_value[:, 1:-1, :].mean(dim=1) if attr_value.shape[1] > 2 else attr_value.mean(dim=1)
                                    cls_embedding = attr_value[:, 0, :]
                                elif len(attr_value.shape) == 2:  # [seq, hidden]
                                    content_embedding = attr_value[1:-1, :].mean(dim=0, keepdim=True) if attr_value.shape[0] > 2 else attr_value.mean(dim=0, keepdim=True)
                                    cls_embedding = attr_value[0:1, :]
                                else:
                                    content_embedding = attr_value
                                    cls_embedding = attr_value
                                
                                return {
                                    'sequence_embedding': content_embedding.cpu().numpy(),
                                    'cls_embedding': cls_embedding.cpu().numpy(),
                                    'hidden_states': attr_value.cpu().numpy(),
                                    'tokens': result.sequence.cpu().numpy() if hasattr(result, 'sequence') else None
                                }
                        except:
                            continue
                
        except Exception as e:
            print(f"âš ï¸ æ–¹æ³•1å¤±è´¥: {str(e)}")
        
        # æ–¹æ³•2: ç›´æ¥ä½¿ç”¨embeddingå±‚
        print("ğŸ”¸ å°è¯•æ–¹æ³•2: ç›´æ¥ä½¿ç”¨embeddingå±‚...")
        try:
            encoded = model.encode(protein)
            tokens = encoded.sequence
            print(f"âœ… Tokenså½¢çŠ¶: {tokens.shape}")
            
            # æŸ¥æ‰¾å¹¶ä½¿ç”¨embeddingå±‚
            embedding_layer = None
            for name, module in model.named_modules():
                if 'embed' in name.lower() and hasattr(module, 'weight'):
                    embedding_layer = module
                    print(f"âœ… æ‰¾åˆ°embeddingå±‚: {name}, æƒé‡å½¢çŠ¶: {module.weight.shape}")
                    break
            
            if embedding_layer is not None:
                # ç›´æ¥é€šè¿‡embeddingå±‚è·å–token embeddings
                token_embeddings = embedding_layer(tokens)
                print(f"âœ… Token embeddingså½¢çŠ¶: {token_embeddings.shape}")
                
                # ç”Ÿæˆåºåˆ—çº§åˆ«embedding
                if len(token_embeddings.shape) == 2:  # [seq_len, hidden_dim]
                    # æ’é™¤ç‰¹æ®Štokensçš„å¹³å‡æ± åŒ–
                    if token_embeddings.shape[0] > 2:
                        content_embedding = token_embeddings[1:-1, :].mean(dim=0, keepdim=True)
                    else:
                        content_embedding = token_embeddings.mean(dim=0, keepdim=True)
                    
                    # CLS token embedding
                    cls_embedding = token_embeddings[0:1, :]
                    
                    return {
                        'sequence_embedding': content_embedding.cpu().numpy(),
                        'cls_embedding': cls_embedding.cpu().numpy(),
                        'token_embeddings': token_embeddings.cpu().numpy(),
                        'tokens': tokens.cpu().numpy()
                    }
            
        except Exception as e:
            print(f"âš ï¸ æ–¹æ³•2å¤±è´¥: {str(e)}")
        
        # æ–¹æ³•3: ä½¿ç”¨tokensåˆ›å»ºç®€å•embedding
        print("ğŸ”¸ æ–¹æ³•3: ä»tokensåˆ›å»ºembedding...")
        try:
            encoded = model.encode(protein)
            tokens = encoded.sequence
            
            # æŸ¥æ‰¾æ¨¡å‹ä¸­çš„embeddingå±‚
            embedding_layer = None
            for name, module in model.named_modules():
                if 'embed' in name.lower() and hasattr(module, 'weight'):
                    embedding_layer = module
                    print(f"âœ… æ‰¾åˆ°embeddingå±‚: {name}")
                    break
            
            if embedding_layer is not None:
                # ä½¿ç”¨æ‰¾åˆ°çš„embeddingå±‚
                token_embeddings = embedding_layer(tokens)
                print(f"âœ… Token embeddings: {token_embeddings.shape}")
                
                # æ·»åŠ batchç»´åº¦å¹¶å¹³å‡æ± åŒ–
                if len(token_embeddings.shape) == 2:
                    sequence_embedding = token_embeddings.mean(dim=0, keepdim=True)
                else:
                    sequence_embedding = token_embeddings.mean(dim=1)
                
                return {
                    'sequence_embedding': sequence_embedding.cpu().numpy(),
                    'token_embeddings': token_embeddings.cpu().numpy(),
                    'tokens': tokens.cpu().numpy()
                }
            else:
                # æœ€åçš„å¤‡é€‰ï¼šè¿”å›tokensçš„one-hotç¼–ç 
                print("âš ï¸ ä½¿ç”¨tokensä½œä¸ºåŸºç¡€ç‰¹å¾")
                vocab_size = tokens.max().item() + 1
                one_hot = torch.zeros(len(tokens), vocab_size, device=tokens.device)
                one_hot[torch.arange(len(tokens)), tokens] = 1
                
                # ç®€å•å¹³å‡
                sequence_embedding = one_hot.mean(dim=0, keepdim=True)
                
                return {
                    'sequence_embedding': sequence_embedding.cpu().numpy(),
                    'tokens': tokens.cpu().numpy(),
                    'one_hot': one_hot.cpu().numpy()
                }
                
        except Exception as e:
            print(f"âš ï¸ æ–¹æ³•3å¤±è´¥: {str(e)}")
        
        print("âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†")
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®€å•ESM3 Embeddingæå–")
    print("="*50)
    
    # 1. å¤„ç†PDBæ–‡ä»¶
    pdb_file = "/content/1qsf.pdb"  # ä¿®æ”¹ä¸ºæ‚¨çš„PDBæ–‡ä»¶è·¯å¾„
    
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¸‹è½½1QSF
    import os
    if not os.path.exists(pdb_file):
        print("ğŸ“¥ ä¸‹è½½1QSF.pdb...")
        import urllib.request
        url = "https://files.rcsb.org/download/1QSF.pdb"
        urllib.request.urlretrieve(url, pdb_file)
    
    sequence, coordinates = get_sequence_from_pdb(pdb_file)
    
    if sequence is None:
        print("âŒ æ— æ³•è·å–åºåˆ—")
        return
    
    # 2. è·å–embedding
    embeddings = get_esm3_embedding(sequence)
    
    if embeddings is None:
        print("âŒ æ— æ³•è·å–embedding")
        return
    
    # 3. ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜ç»“æœ...")
    
    # ä¿å­˜ä¸ºnumpyæ–‡ä»¶
    result_data = {
        'sequence': sequence,
        'coordinates': coordinates,
        **embeddings
    }
    
    np.save('protein_embeddings.npy', result_data, allow_pickle=True)
    print("âœ… å·²ä¿å­˜åˆ°: protein_embeddings.npy")
    
    # ä¿å­˜åºåˆ—çº§åˆ«embeddingä¸ºCSV
    if 'sequence_embedding' in embeddings:
        df = pd.DataFrame(
            embeddings['sequence_embedding'], 
            columns=[f'dim_{i}' for i in range(embeddings['sequence_embedding'].shape[1])]
        )
        df.to_csv('sequence_embedding.csv', index=False)
        print("âœ… åºåˆ—embeddingå·²ä¿å­˜åˆ°: sequence_embedding.csv")
    
    print("\nğŸ‰ å®Œæˆ!")
    print(f"ğŸ“Š ç»“æœåŒ…å«: {list(embeddings.keys())}")

if __name__ == "__main__":
    main() 